{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 11:39:58.507547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745581198.517241 1626713 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745581198.520196 1626713 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 11:39:58.531042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d222323c7fc44558938a65f853b81211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3023.7419147491455\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoProcessor, BitsAndBytesConfig, get_scheduler,Gemma3ForConditionalGeneration\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import here opencv\n",
    "\n",
    "\n",
    "\n",
    "# device_name = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "# device = torch.device(device_name)\n",
    "\n",
    "DEFAULT_MODEL = \"google/gemma-3-4b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                                load_in_4bit=True,\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_storage=torch.bfloat16,\n",
    "                                )\n",
    "\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation='eager',\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    "    torch_dtype=torch.bfloat16\n",
    "    \n",
    ")\n",
    "\n",
    "print(model.get_memory_footprint()/(1024*1024)) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Multi Lingual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"OdiaGenAI/hardcode_odia_qa_105\")\n",
    "dataset = load_dataset('OdiaGenAI/odia_domain_context_train_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଓଡ଼ିଶାର ଉଦୟଗିରି ଏବଂ ଖଣ୍ଡଗିରି ଗୁମ୍ଫାର ଇତିହାସ କ’ଣ?',\n",
       " 'input': '',\n",
       " 'output': 'ଉଦୟଗିରି ଏବଂ ଖଣ୍ଡଗିରି ଗୁମ୍ଫା ଭାରତର ଓଡ଼ିଶା ରାଜ୍ୟରେ ଅବସ୍ଥିତ ପ୍ରାଚୀନ ଗୁମ୍ଫା ପରିସରର ଏକ ସମୂହ। ଏହି ଗୁମ୍ଫା ଗୁଡିକ ଖ୍ରୀଷ୍ଟପୂର୍ବ ଦ୍ୱିତୀୟ ଶତାବ୍ଦୀ ଏବଂ ପ୍ରଥମ ଶତାବ୍ଦୀ ମଧ୍ୟରେ ଜୈନ ସନ୍ନ୍ୟାସୀମାନଙ୍କ ଦ୍ୱାରା ବାଲୁକା ପଥରରେ ଖୋଦିତ ହୋଇଥିବା ଜଣାଯାଏ।\\nଉଦୟଗିରି ଗୁମ୍ଫା ଗୁଡିକ ବୃହତ ଏବଂ ଅଧିକ ଜଟିଳ ହୋଇଥିବାବେଳେ ଖଣ୍ଡଗିରି ଗୁମ୍ଫା ଗୁଡିକ ଛୋଟ ଏବଂ ଡିଜାଇନ ଦୃଷ୍ଟିରୁ ସରଳ। ଏହି ପ୍ରାଚୀନ ଗୁମ୍ଫା ଗୁଡିକ ପ୍ରାଚୀନ ଭାରତୀୟ ସଭ୍ୟତାର ସ୍ଥାପତ୍ୟ କୌଶଳ ଏବଂ କଳାତ୍ମକ ଶ୍ରେଷ୍ଠତାର ପ୍ରମାଣ।\\nବିଗତ ବର୍ଷମାନଙ୍କରେ ଏହି ଗୁମ୍ଫା ସାରା ବିଶ୍ୱରୁ ପର୍ଯ୍ୟଟକଙ୍କ ଆକର୍ଷଣ କେନ୍ଦ୍ର ପାଲଟିଛି ଏବଂ ଭାରତୀୟ ପ୍ରତ୍ନତାତ୍ୱିକ ସର୍ବେକ୍ଷଣ ଦ୍ୱାରା ସଂରକ୍ଷିତ ହୋଇଛି। ଭାରତର ସମୃଦ୍ଧ ସାଂସ୍କୃତିକ ଐତିହ୍ୟ ପ୍ରତି ଆଗ୍ରହୀ ଯେକୌଣସି ବ୍ୟକ୍ତି ଏହି ଗୁମ୍ଫାକୁ ଦେଖିବା ଉଚିତ।'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(tokenizer(example['instruction']+ example['output'])['input_ids']) for example in dataset['train']]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Tokenized Output Lengths\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      # {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": sample['instruction']},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=['output','instruction','input'],batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "# dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "# # Print formatted user prompt\n",
    "# print(dataset[\"train\"][345][\"messages\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nଓଡ଼ିଶାର ଅର୍ଥନୀତି ଉପରେ ପର୍ଯ୍ୟଟନର କିଭଳି ପ୍ରଭାବ ପଡ଼ିଛି?<end_of_turn>\\n<start_of_turn>model\\nବିଗତ କିଛି ବର୍ଷ ମଧ୍ୟରେ ପର୍ଯ୍ୟଟନ ଶିଳ୍ପ ଦ୍ୱାରା ଓଡ଼ିଶାର ଅର୍ଥନୀତି ବ୍ୟାପକ ଭାବେ ପ୍ରଭାବିତ ହୋଇଛି। ରାଜ୍ୟରେ ପ୍ରାକୃତିକ ସୌନ୍ଦର୍ଯ୍ୟ ଏବଂ ସାଂସ୍କୃତିକ ଐତିହ୍ୟ ସ୍ଥାନ ରହିଛି, ଯାହା ଦେଶ ଏବଂ ଦେଶ ବାହାରର ପର୍ଯ୍ୟଟକଙ୍କ ପାଇଁ ଆକର୍ଷଣୀୟ ହୋଇପାରିଛି।\\nପର୍ଯ୍ୟଟନ କ୍ଷେତ୍ରରେ କାର୍ଯ୍ୟ କରୁଥିବା ଲୋକମାନଙ୍କ ପାଇଁ ନିଯୁକ୍ତି ସୁଯୋଗ ଏବଂ ଆୟ ବୃଦ୍ଧି ପାଇଛି, ଯେଉଁଥିରେ ଆତିଥ୍ୟ, ଟୁର ଅପରେଟର ଏବଂ ପରିବହନ ଶିଳ୍ପ ସାମିଲ ରହିଛି। ପର୍ଯ୍ୟଟନ ଶିଳ୍ପ ସ୍ଥାନୀୟ ଖାଦ୍ୟ, ହସ୍ତଶିଳ୍ପ ଏବଂ ସାଂସ୍କୃତିକ ଗତିବିଧିକୁ ପ୍ରୋତ୍ସାହିତ କରିବାରେ ସହାୟତା କରିଛି, ଯାହାକି ଓଡ଼ିଶାର ସାମଗ୍ରିକ ଅର୍ଥନୀତି ଉପରେ ସକାରାତ୍ମକ ପ୍ରଭାବ ପକାଇଛି।\\nପର୍ଯ୍ୟଟକମାନଙ୍କ ପାଇଁ ଉତ୍ତମ ଅନୁଭୂତି ସୁନିଶ୍ଚିତ କରିବା ପାଇଁ ଭିତ୍ତିଭୂମି ବିକାଶ ଉପରେ ମଧ୍ୟ ସରକାର ଗୁରୁତ୍ୱ ଦେଉଛନ୍ତି। ସଡ଼କ ଏବଂ ସାର୍ବଜନୀନ ପରିବହନ ବ୍ୟବସ୍ଥାରେ ସୁଧାର କରାଯାଇଛି, ଯେତେବେଳେ କି ରାଜ୍ୟର ଅନେକ କ୍ଷେତ୍ରରେ ବିଭିନ୍ନ ଇକୋ-ଟୁରିଜମ ପ୍ରକଳ୍ପର ବିକାଶ କରାଯାଇଛି।\\nଶେଷରେ ପର୍ଯ୍ୟଟନ ଓଡ଼ିଶାର ଆର୍ଥିକ ବିକାଶକୁ ତ୍ୱରାନ୍ୱିତ କରିବାରେ ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ଭୂମିକା ନିର୍ବାହ କରିଛି ଏବଂ ରାଜ୍ୟବାସୀଙ୍କ ପାଇଁ ଅନେକ ଲାଭ ଆଣିପାରିଛି।<end_of_turn>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(dataset['train'][0]['messages'],tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Training using TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "torch_dtype = torch.bfloat16\n",
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-text-to-sql\",         # directory to save and repository id\n",
    "    max_seq_length=512,                     # max sequence length for model and packing of the dataset\n",
    "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2064' max='2064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2064/2064 1:02:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>5.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>5.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>4.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.782600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>4.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>4.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>4.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>5.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>4.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>4.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>4.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>4.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>4.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>4.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>4.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>4.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>4.872900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>4.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>4.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>4.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>4.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>4.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>4.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>3.925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>4.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>3.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>4.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>4.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>3.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>4.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>3.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>4.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>4.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>3.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>4.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>4.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>4.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>4.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>3.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>4.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>4.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>4.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>4.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>3.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>4.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>4.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>3.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>3.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>3.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>4.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>3.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>3.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>4.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>3.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>4.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>4.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>3.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>3.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>4.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>4.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>3.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>3.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>3.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>4.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>4.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>4.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>3.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>4.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>3.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>4.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>4.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>3.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>3.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>4.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>3.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>4.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>3.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>3.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>3.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>3.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>3.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>3.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>3.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>3.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>3.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>3.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>3.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>3.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>3.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>3.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>3.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>3.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>3.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>3.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>3.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>3.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>3.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>3.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>3.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>3.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>3.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>3.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>3.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>3.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>3.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>3.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>3.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>3.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>3.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>3.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>3.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>3.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>3.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>3.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>3.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>3.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>3.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>3.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>3.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>3.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>3.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>3.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>3.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>3.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>3.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>3.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>3.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>3.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>3.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>3.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>3.317100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/models/Mohan-diffuser/gemma-text-to-sql/preupload/main (Request ID: Root=1-680b835b-00f55311711422712ea6f078;3de44368-d484-46cb-9f99-ff2f9c551958)\n\nInvalid credentials in Authorization header",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/Mohan-diffuser/gemma-text-to-sql/preupload/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the final model again to the Hugging Face Hub\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/trainer.py:3906\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n\u001b[0;32m-> 3906\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel save\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/trainer.py:4841\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   4839\u001b[0m \u001b[38;5;66;03m# Wait for the current upload to be finished.\u001b[39;00m\n\u001b[1;32m   4840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish_current_push()\n\u001b[0;32m-> 4841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_as_future\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPREFIX_CHECKPOINT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1624\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4934\u001b[0m, in \u001b[0;36mHfApi.upload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[0m\n\u001b[1;32m   4930\u001b[0m commit_operations \u001b[38;5;241m=\u001b[39m delete_operations \u001b[38;5;241m+\u001b[39m add_operations\n\u001b[1;32m   4932\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m commit_message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload folder using huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4934\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4937\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4944\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4946\u001b[0m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[1;32m   4947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1624\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4193\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[1;32m   4191\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[0;32m-> 4193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4195\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[1;32m   4199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[1;32m   4202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4204\u001b[0m files_to_copy \u001b[38;5;241m=\u001b[39m _fetch_files_to_copy(\n\u001b[1;32m   4205\u001b[0m     copies\u001b[38;5;241m=\u001b[39mcopies,\n\u001b[1;32m   4206\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4210\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   4211\u001b[0m )\n\u001b[1;32m   4212\u001b[0m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4416\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4414\u001b[0m \u001b[38;5;66;03m# Check which new files are LFS\u001b[39;00m\n\u001b[1;32m   4415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4416\u001b[0m     \u001b[43m_fetch_upload_modes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4417\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_additions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgitignore_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4427\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:680\u001b[0m, in \u001b[0;36m_fetch_upload_modes\u001b[0;34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001b[0m\n\u001b[1;32m    672\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgitIgnore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gitignore_content\n\u001b[1;32m    674\u001b[0m resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/preupload/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    676\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    677\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    678\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_pr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    679\u001b[0m )\n\u001b[0;32m--> 680\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m preupload_info \u001b[38;5;241m=\u001b[39m _validate_preupload_info(resp\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    682\u001b[0m upload_modes\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]: file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muploadMode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m preupload_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/Mohan-diffuser/gemma-text-to-sql/preupload/main (Request ID: Root=1-680b835b-00f55311711422712ea6f078;3de44368-d484-46cb-9f99-ff2f9c551958)\n\nInvalid credentials in Authorization header"
     ]
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f63dbd5dcf247d9b6343f2911d94b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1745581212.rack-gpu-02.1626713.0:   0%|          | 0.00/63.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0b2137ba2046c79cb58cae0d2136b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3790099a64e445d696fd0f2220578f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 13:05:25.680784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745586325.690528 1636331 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745586325.693544 1636331 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 13:05:25.704657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74954a30119e4219b426192a85124460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoProcessor, BitsAndBytesConfig, get_scheduler,Gemma3ForConditionalGeneration\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "model_id= \"google/gemma-3-4b-it\"\n",
    "# Load Model base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                                load_in_4bit=True,\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_storage=torch.bfloat16,\n",
    "                                )\n",
    "\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation='eager',\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    "    torch_dtype=torch.bfloat16\n",
    "    \n",
    ").eval()\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(model, 'gemma-text-to-sql').to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gemma-text-to-sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(model,idx=5,disable_lora=False):\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    sample=dataset['train'][idx]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to('cuda').to(torch.float16)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if disable_lora:\n",
    "        with model.disable_adapter():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=256,\n",
    "                repetition_penalty=1.3,\n",
    "                temperature=0.7,         # Optional: smooth randomness\n",
    "                top_k=50,                # Optional: top-k sampling\n",
    "                top_p=0.9                # Optional: nucleus sampling\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=256,\n",
    "        repetition_penalty=1.3,\n",
    "        temperature=0.2,         # Optional: smooth randomness\n",
    "        top_k=50,                # Optional: top-k sampling\n",
    "        top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred,true=generate_eval(peft_model,idx=56,disable_lora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "ପ୍ରତିବର୍ଷ ଓଡ଼ିଶା ପୁରସ୍କାର ବିଜେତାମାନଙ୍କୁ କିପରି ଚୟନ କରାଯାଇଥାଏ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ଓଡ଼ିଶା ସରକାର ପ୍ରତ୍ୟକ୍ଷ ଭାବରେ ଏହି ଉତ୍ସବ ଆୟୋଜନ କରିଥାନ୍ତି। ଅନ୍ୟ ରାଜ୍ୟଗୁଡିକଠାରୁ ଯୌନ ନିମ୍ନସ୍ତରୀୟ ଗତିବିଧିକୁ ଦୂର କରିବା ପାଇଁ ମଧ୍ୟ ଜଣେ ବ୍ୟକ୍ତିକୁ ଚୟନ କରିବା ପାଇଁ ଏହା ଏକ ପ୍ରଭାବଶାଳୀ ପ୍ରଦର୍ଷ୍ଟନୀ ହୋଇଛି, କିନ୍ତୁ ଏହାର ଶୈଳୀରେ ପରିବର୍ତ୍ତନ ଆସିଛି।\n",
      "ଅନ୍ୟ ଏକ ଖବରପତ୍ରରେ ଘଟିଥିବା ଅନୁଯାୟୀ, ପ୍ରତିବର୍ଷ ଓଡ଼ିଶା ସରକାର ‘ଓଡ଼ିଆ’ ରଙ୍ଗରେ ପାଞ୍ଚ ଜଣଙ୍କୁ ଚୟନ କରିଛନ୍ତି ଏବଂ ସେମାନଙ୍କୁ 100-250 ଲକ୍ଷ ଋଣ ପ୍ରଦାନ କରିଛନ୍ତି।\n",
      "ଓଡ଼ିଶା ପୁର\n",
      "******************************\n",
      "କ୍ରୀଡ଼ା, କଳା, ସାହିତ୍ୟ, ସଂସ୍କୃତି, ବିଜ୍ଞାନ ଓ ପ୍ରଯୁକ୍ତି ଏବଂ ସାମାଜିକ ସେବା ଭଳି ବିଭିନ୍ନ କ୍ଷେତ୍ରରେ ଉଲ୍ଲେଖନୀୟ ସଫଳତା, ନବୋନ୍ମେଷ ଏବଂ ଉତ୍କର୍ଷକୁ ସ୍ୱୀକୃତି ପ୍ରଦାନ କରିବା ପାଇଁ ଏହି ପୁରସ୍କାର ପ୍ରଦାନ କରାଯାଇଥାଏ।\n",
      "ଚୟନ ପ୍ରକ୍ରିୟାରେ ଅନେକ ପର୍ଯ୍ୟାୟରେ ନାମାଙ୍କନ ଆହ୍ୱାନ କରାଯାଇଥାଏ, ଯାହା ପରେ ଯୋଗ୍ୟ ପ୍ରାର୍ଥୀମାନଙ୍କୁ ନିର୍ଦ୍ଦିଷ୍ଟ ମାନଦଣ୍ଡ ଏବଂ ମାନଦଣ୍ଡ ଆଧାରରେ ଚୟନ କରାଯାଇଥାଏ। ଏହାପରେ ଚୟନ କରାଯାଇଥିବା ପ୍ରାର୍ଥୀମାନଙ୍କୁ ବିଚାରପତିମାନଙ୍କ ଏକ ପ୍ୟାନେଲ ଦ୍ୱାରା ମୂଲ୍ୟାଙ୍କନ କରାଯାଇଥାଏ, ଯେଉଁମାନେ ପ୍ରତ୍ୟେକ ପ୍ରାର୍ଥୀଙ୍କ ପ୍ରୋଫାଇଲ, ଉପଲବ୍ଧି ଏବଂ ସେମାନଙ୍କ ନିଜ ନିଜ କ୍ଷେତ୍ରରେ ଯୋଗଦାନ ବିଷୟରେ ଧ୍ୟାନପୂର୍ବକ ସମୀକ୍ଷା କରିଥାନ୍ତି।\n",
      "ଏଠାରେ ଉଲ୍ଲେଖନୀୟ ଯେ ଚୟନ ପ୍ରକ୍ରିୟା ସ୍ୱଚ୍ଛ ଏବଂ ନିରପେକ୍ଷ ଏବଂ ବିଚାରପତିମାନଙ୍କ ପ୍ୟାନେଲ ନିଜ ନିଜ କ୍ଷେତ୍ରରେ ପ୍ରତିଷ୍ଠିତ ଏବଂ ପ୍ରତିଷ୍ଠିତ ବ୍ୟକ୍ତିବିଶେଷଙ୍କୁ ନେଇ ଗଠିତ ହୋଇଥାଏ। ଏହା ସୁନିଶ୍ଚିତ କରିଥାଏ ଯେ ପୁରସ୍କାର ଉତ୍କର୍ଷ ଏବଂ ଯୋଗ୍ଯତାର ପ୍ରକୃତ ପ୍ରତିଫଳନ ଏବଂ ବିଜେତାମାନେ ବାସ୍ତବରେ ମାନ୍ୟତା ଏବଂ ପ୍ରଶଂସାର ହକଦାର।\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print('*'*30)\n",
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Pytorch Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"what is capital of france?\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "# outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<bos><start_of_turn>user\\nYou are a helpful assistant\\n\\nwhat is capital of france?<end_of_turn>\\n<start_of_turn>model\\nThe capital of France is **Paris**. \\n\\nIt's a beautiful and historic city, known for landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral! \\n\\nDo you want to know anything more about Paris, or perhaps another city?<end_of_turn>\"]\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Object for Pytorch-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "from os import truncate\n",
    "\n",
    "\n",
    "class LlamaDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        question=sample['instruction']\n",
    "        answer = sample['output']\n",
    "        prompt = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "        full_text = prompt+f'''{answer}<end_of_turn>'''\n",
    "\n",
    "        tokenized = tokenizer(full_text,add_special_tokens=False,truncation=True,max_length=250)\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        # Tokenize just the prompt to get the split point\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        answer_start = len(prompt_ids)\n",
    "\n",
    "        # Mask everything before answer_start\n",
    "        labels = [-100] * answer_start + input_ids[answer_start:]\n",
    "        # Mask out padding as well\n",
    "        labels = [\n",
    "            label if token != tokenizer.pad_token_id else -100\n",
    "            for label, token in zip(labels, input_ids)\n",
    "        ]\n",
    "    \n",
    "        return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def llama_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the max length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": labels_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Assume dataset['train'] is the full dataset you want to split\n",
    "full_dataset = dataset['train']\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = random_split(full_dataset, [0.95, 0.05])\n",
    "\n",
    "# Wrap in your custom Dataset class\n",
    "train_dataset = LlamaDataset(train_data)\n",
    "test_dataset = LlamaDataset(test_data)\n",
    "\n",
    "# DataLoaders with custom collate_fn\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=llama_collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=llama_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 88])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower.vision_model.embeddings.patch_embedding.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.embeddings.patch_embedding.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.embeddings.position_embedding.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.post_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.post_layernorm.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "multi_modal_projector.mm_input_projection_weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "multi_modal_projector.mm_soft_emb_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of `prepare_model_for_kbit_training`\n",
    "\n",
    "When you load a model in 4-bit or 8-bit precision using bitsandbytes, some layers (like LayerNorm) still remain in full precision (float32), and certain operations (like weight updates) can be unstable or incompatible if done blindly on quantized weights.\n",
    "\n",
    "- Casts `LayerNorm` layers to `float32`\n",
    "- Sets `requires_grad=False` for all model parameters\n",
    "- Wraps the output layer (like `lm_head`) in `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 42,734,080 || all params: 4,342,813,552 || trainable%: 0.9840\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vision_tower.vision_model.embeddings.patch_embedding.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.embeddings.patch_embedding.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.embeddings.position_embedding.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.post_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.post_layernorm.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.multi_modal_projector.mm_input_projection_weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.multi_modal_projector.mm_soft_emb_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.lm_head.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.lm_head.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = '''ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର କିପରି ମିଳିମିଶି କାର୍ଯ୍ୟ କରିପାରିବେ?'''\n",
    "# answer = '''ଯେକୌଣସି ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ମିଳିତ ପ୍ରୟାସର ଆବଶ୍ୟକତା ରହିଛି। ଓଡ଼ିଶାର ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ବିସ୍ତୃତ ରଣନୀତି ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିବା ଲାଗି ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିପାରିବେ।\n",
    "# ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହିତ ମିଶି କାମ କରିବାର ଗୋଟିଏ ଉପାୟ ହେଲା ଘରୋଇ ନିବେଶ ପାଇଁ ଅନୁକୂଳ ବାତାବରଣ ସୃଷ୍ଟି କରିବା, ଏଥିରେ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ଟିକସ ଏବଂ ନିୟାମକ ପ୍ରତିବନ୍ଧକକୁ ହ୍ରାସ କରିବା, ଏହା ବ୍ୟତୀତ ସରକାର ଟିକସ ରିହାତି, ସବସିଡି ଏବଂ ପର୍ଯ୍ୟଟନ ବିକାଶ ପ୍ରକଳ୍ପ ପାଇଁ ଜମି ଆଦି ପ୍ରୋତ୍ସାହନ ମଧ୍ୟ ପ୍ରଦାନ କରିପାରିବେ।\n",
    "# ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ନୂତନ ପର୍ଯ୍ୟଟନ ଉତ୍ପାଦ ପ୍ରସ୍ତୁତ କରିପାରିବେ ଯାହା ଉଭୟ ଘରୋଇ ଏବଂ ଅନ୍ତର୍ଜାତୀୟ ପର୍ଯ୍ୟଟକଙ୍କ ଆବଶ୍ୟକତା ପୂରଣ କରିପାରିବ।\n",
    "# ସରକାର ମଧ୍ୟ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ପ୍ରଯୁକ୍ତିର ଉପଯୋଗ କରିପାରିବେ। ଉଦାହରଣ ସ୍ୱରୂପ, ସରକାର ପର୍ଯ୍ୟଟନ ସ୍ଥଳକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ଏବଂ ସମ୍ଭାବ୍ୟ ପର୍ଯ୍ୟଟକମାନଙ୍କ ସହିତ ଯୋଡ଼ିବା ଲାଗି ସୋସିଆଲ ମିଡିଆ ପ୍ଲାଟଫର୍ମର ଉପଯୋଗ କରିପାରିବେ। ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନ ଆକର୍ଷଣ ଏବଂ ଅନୁଭବ ପ୍ରଦର୍ଶିତ କରିବା ଲାଗି ଏକ ଅନଲାଇନ ପ୍ଲାଟଫର୍ମ ପ୍ରତିଷ୍ଠା କରିବା ଦ୍ୱାରା ଅଧିକ ପର୍ଯ୍ୟଟକଙ୍କୁ ଆକର୍ଷିତ କରିବାରେ ସହାୟତା ମିଳିପାରିବ।\n",
    "# ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିବାକୁ ପଡିବ ଯେପରିକି ପର୍ଯ୍ୟଟନ ଗତିବିଧି ଦ୍ୱାରା ପର୍ଯ୍ୟାବରଣର କ୍ଷୟ କିମ୍ବା ସ୍ଥାନୀୟ ସମ୍ପ୍ରଦାୟର କ୍ଷତି ନ ହେଉ।\n",
    "# ଶେଷରେ, ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ଅନୁକୂଳ ପରିବେଶ ସୃଷ୍ଟି କରିବା ଆବଶ୍ୟକ ଏବଂ ଏହା ସୁନିଶ୍ଚିତ କରିବା ଉଚିତ ଯେ ବିକାଶ ସ୍ଥାୟୀ ହେବ। ” ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରି ସେମାନେ ପର୍ଯ୍ୟଟନ ରଣନୀତିକୁ ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିପାରିବେ ଯାହା କେବଳ ପର୍ଯ୍ୟଟନ ଉଦ୍ୟୋଗ ନୁହେଁ ବରଂ ସ୍ଥାନୀୟ ଗୋଷ୍ଠୀ ଏବଂ ପରିବେଶକୁ ମଧ୍ୟ ଲାଭାନ୍ୱିତ କରିବ।'''\n",
    "\n",
    "# tokenized_text = tokenizer(answer).input_ids\n",
    "# print(len(tokenized_text))\n",
    "# for idx in range(len(tokenized_text)):\n",
    "#     clear_output(wait=True)\n",
    "#     print(tokenizer.decode(tokenized_text[0:idx]))\n",
    "#     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the LLAMA model on a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"user\\nଶରୀର ଅସନ୍ତୁଷ୍ଟିର ସଂଜ୍ଞା କ 'ଣ?\\nmodel\\nଜଣେ ବ୍ୟକ୍ତି ନିଜ ଛବି କିମ୍ବା ଚେହେରାରେ ଅସନ୍ତୁଷ୍ଟ ହେବାର ଧାରଣା\",\n",
       " 'user\\nକିପରି ଚାରି ସ୍ପିଡ୍ ବିଶିଷ୍ଟ ଇଞ୍ଜିନ ଚଳାଚଳ କରିପାରିବ?\\nmodel\\nସ୍ଲିପ ରିଙ୍ଗ ବ୍ୟବହାର କରି ମୋଟର ବଦଳାଯାଇପାରିବ।']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "tokenizer.batch_decode(batch['input_ids'],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(model,idx=5,disable_lora=False):\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    sample=dataset['train'][idx]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to(device).to(torch.float16)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if disable_lora:\n",
    "        with model.disable_adapter():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=256,\n",
    "                repetition_penalty=1.3,\n",
    "                temperature=0.7,         # Optional: smooth randomness\n",
    "                top_k=50,                # Optional: top-k sampling\n",
    "                top_p=0.9                # Optional: nucleus sampling\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=256,\n",
    "        repetition_penalty=1.3,\n",
    "        temperature=0.7,         # Optional: smooth randomness\n",
    "        top_k=50,                # Optional: top-k sampling\n",
    "        top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?',\n",
       " 'input': '',\n",
       " 'output': 'ଏକାଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମାଣ କରାଯାଇଥିବା ରାଜାରାଣୀ ମନ୍ଦିର ଭୁବନେଶ୍ୱର ସହରର ଏକ ଲୋକପ୍ରିୟ ପର୍ଯ୍ୟଟନସ୍ଥଳୀ।\\nଏକାଦଶ ଶତାବ୍ଦୀରେ ଏହି ଅଂଚଳରେ ରାଜାରାଣୀ ରାଜବଂଶ ଶାସନ କରିଥିଲେ। ବିଶ୍ୱାସ କରାଯାଏ ଯେ ଏହି ମନ୍ଦିର ପ୍ରଥମେ ଭଗବାନ ଶିବଙ୍କୁ ସମର୍ପିତ ଥିଲା, କିନ୍ତୁ ପରେ ଏହା ଭଗବାନ ବିଷ୍ଣୁଙ୍କ ମନ୍ଦିର ପାଲଟିଥିଲା।\\nରାଜାରାଣୀ ମନ୍ଦିରର ସ୍ଥାପତ୍ୟ ଅଦ୍ୱିତୀୟ, କାରଣ ଏଠାରେ କୌଣସି ପବିତ୍ର ସ୍ଥାନ କିମ୍ବା କେନ୍ଦ୍ରୀୟ ମନ୍ଦିର ନାହିଁ, ଯାହାକି ହିନ୍ଦୁ ମନ୍ଦିରର ଏକ ବିଶେଷ ବୈଶିଷ୍ଟ୍ୟ, ଏହା ପରିବର୍ତ୍ତେ ମନ୍ଦିରର ଚାରିପଟେ ଛୋଟ ଛୋଟ ମନ୍ଦିର ରହିଛି ଯେଉଁଥିରେ ବିଭିନ୍ନ ଦେବଦେବୀଙ୍କ ମୂର୍ତ୍ତି ରହିଛି।\\nଏହି ମନ୍ଦିରର ଅନ୍ୟ ଏକ ବିଶେଷତ୍ୱ ହେଉଛି ଏହାର ସୁସଜ୍ଜିତ ଭାସ୍କର୍ଯ୍ୟ ଏବଂ ଖୋଦାଇ କାର୍ଯ୍ୟ। ଏହି ମନ୍ଦିରରେ ଦେବ-ଦେବୀ ଏବଂ ପୌରାଣିକ ପ୍ରାଣୀମାନଙ୍କ ଜଟିଳ ଚିତ୍ରଣ ରହିଛି, ଯାହା ଏହାକୁ କଳା ଏବଂ ସ୍ଥାପତ୍ୟର ଉତ୍ସାହୀମାନଙ୍କ ପାଇଁ ଏକ ଲୋକପ୍ରିୟ ଗନ୍ତବ୍ୟ ସ୍ଥଳୀରେ ପରିଣତ କରିଛି।\\nବିଗତ ବର୍ଷମାନଙ୍କରେ, ରାଜାରାଣୀ ମନ୍ଦିର ଭୁବନେଶ୍ୱରର ଏକ ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ସାଂସ୍କୃତିକ ସ୍ଥଳୀରେ ପରିଣତ ହୋଇଛି ଏବଂ ଏହି କ୍ଷେତ୍ରର ସମୃଦ୍ଧ ଇତିହାସ ଏବଂ ସାଂସ୍କୃତିକ ଐତିହ୍ୟର ଏକ ଉଦାହରଣ।'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ଭୂବାନ୍ ଏକ ପ୍ରମୁଖ ପୋତାଲ ସହସ୍ପଦ ଗୃହ ଅନ୍ତର୍ଗତ ଭୂବାନ୍ ରହିଥିଲା, ଯେଉଁଠ୍ୟ ବିଷୟରେ ଜାନଙ୍କ ଦୁଷ୍ଟିକ ଆଚାର୍କ ଶ୍ରୀ ନାରଡ଼ଙ୍ଗ ବାଳୟାଞ୍ କରିଛନ୍ତି। ହରିଧର ଓ ଶոնକୁଆଙ୍କୁ ଯାଇଥଣ୍ ଲେକ୍ଵିଂ ଟିଏମ୍ପﻞ - ଉତ୍ତରୀୟାଙ୍କୁ ଯାଇଛି ।\n",
      "\n",
      "ଭୂବାନ୍ ରହିଥିଲା ଦୁଃଖୀ ସାମନ୍ତ ମାଘାଦ aceptación ଧନ୍ଯା ଧ୍ୟାନ ଦିଅନା୽ିଲେ , ଆଚାର୍କ ଶ୍ରୀ ନାରଡ଼ଙ୍ଗ ବାଳୟାଞ୍ ଫରମାльным ଐତିହିକ ତୁଝି ଢାଙ୍କି ରାଜାମାନଙ୍କ ସାମନ୍ତ୍ରଣ କରି ଶିଶି ୧୪୬୦-୧୫୩૦ ସ outstretched । ଆଚାର୍କ ଶ୍ରୀ ନାରଡ଼ଙ୍ગ ବାଳୟା\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Loss: 105.1638\n",
      "Step 2/1000, Loss: 111.7463\n",
      "Step 3/1000, Loss: 102.6180\n",
      "Step 4/1000, Loss: 108.1015\n",
      "Step 5/1000, Loss: 1462.9916\n",
      "Step 6/1000, Loss: 1617.7755\n",
      "Step 7/1000, Loss: 1479.4073\n",
      "Step 8/1000, Loss: 1476.1635\n",
      "Step 9/1000, Loss: 1018.2379\n",
      "Step 10/1000, Loss: 614.6854\n",
      "Step 11/1000, Loss: 709.9644\n",
      "Step 12/1000, Loss: 659.9238\n",
      "Step 13/1000, Loss: 3906.9778\n",
      "Step 14/1000, Loss: 4277.8843\n",
      "Step 15/1000, Loss: 4173.8867\n",
      "Step 16/1000, Loss: 4125.7783\n",
      "Step 17/1000, Loss: 5786.9146\n",
      "Step 18/1000, Loss: 4640.8599\n",
      "Step 19/1000, Loss: 5666.2646\n",
      "Step 20/1000, Loss: 4517.5142\n",
      "Step 21/1000, Loss: 1381.9725\n",
      "Step 22/1000, Loss: 1467.5356\n",
      "Step 23/1000, Loss: 1521.2346\n",
      "Step 24/1000, Loss: 1998.3424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     41\u001b[0m running_train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:1326\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mignore_index, labels)\n\u001b[1;32m   1323\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_causal_mask(\n\u001b[1;32m   1324\u001b[0m     attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n\u001b[1;32m   1325\u001b[0m )\n\u001b[0;32m-> 1326\u001b[0m outputs: CausalLMOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m   1340\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:942\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    939\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    940\u001b[0m )\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:722\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    709\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    710\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m         last_cache_position,\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_cache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:437\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[1;32m    436\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m--> 437\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_feedforward_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    440\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:135\u001b[0m, in \u001b[0;36mGemma3RMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 135\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst Gemma3 is (x * w).to(float16)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/models/gemma3/modeling_gemma3.py:132\u001b[0m, in \u001b[0;36mGemma3RMSNorm._norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "gradient_accumulation_steps = 4\n",
    "max_steps=1000\n",
    "max_loss = 1e9\n",
    "\n",
    "# Track losses\n",
    "train_losses = []\n",
    "running_train_loss=[]\n",
    "val_losses = []\n",
    "steps = []\n",
    "val_steps=[]\n",
    "\n",
    "# Define optimizer\n",
    "params_to_optimize = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "optimizer = Adam8bit(params_to_optimize, lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_steps,\n",
    ")\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "global_step= 0\n",
    "\n",
    "while global_step< max_steps:\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        model.config.use_cache = False\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'].to('cuda'), attention_mask=batch['attention_mask'].to('cuda'), labels=batch['labels'].to('cuda'))\n",
    "        loss = outputs.loss\n",
    "        running_train_loss.append(loss.item())\n",
    "        loss = loss / gradient_accumulation_steps  # Normalize loss\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        global_step += 1\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "        \n",
    "        print(f\"Step {global_step}/{max_steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            flush()\n",
    "            # Store train loss\n",
    "            train_losses.append(np.mean(running_train_loss[-20:]))\n",
    "            steps.append(global_step)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            count = 0\n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(test_dataloader, total=len(test_dataloader), leave=True)\n",
    "                for val_batch in pbar:\n",
    "                    val_outputs = model(\n",
    "                        input_ids=val_batch['input_ids'].to('cuda'),\n",
    "                        attention_mask=val_batch['attention_mask'].to('cuda'),\n",
    "                        labels=val_batch['labels'].to('cuda')\n",
    "                    )\n",
    "                    total_val_loss += val_outputs.loss.item()\n",
    "                    count += 1\n",
    "                    pbar.set_postfix({'val_loss': f'{(total_val_loss / count):.4f}'})\n",
    "            avg_val_loss = total_val_loss / count\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_steps.append(global_step)\n",
    "            print(f\"[Eval at step {global_step}] Val Loss: {avg_val_loss:.4f}\")\n",
    "            model.train()\n",
    "            # Plotting losses\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(running_train_loss[::len(val_steps)], label='Train Loss')\n",
    "            plt.plot(val_steps, val_losses, label='Val Loss')\n",
    "            plt.ylim([0,3])\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"train_val_loss_plot.png\")\n",
    "            plt.show()\n",
    "            \n",
    "        # if global_step % 20 == 0:\n",
    "        #     pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "        #     print('*'*20,step+1,'*'*20)\n",
    "        #     print(\"Predictions:\", pred)\n",
    "        #     print('*'*20,'end','*'*20)\n",
    "            \n",
    "        # if loss.item() < max_loss:\n",
    "        #     model.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')\n",
    "        #     max_loss = loss.item()\n",
    "                \n",
    "         \n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 69 ********************\n",
      "Predictions: <bos><bos><start_of_turn>user\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ି دି.ତ ହା ିିୁ  ଧ,ଶ ସ ମ୍ ାର ୀ୍ୟ ପୋ ଶ୍ୟ, ୍ୟା୍େିିିକ  ପL  େ 'ର୍ ହ,ା<end_of_turn>\n",
      "******************** end ********************\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "print('*'*20,step+1,'*'*20)\n",
    "print(\"Predictions:\", pred)\n",
    "print('*'*20,'end','*'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the LoRA and saving the Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_adapter.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the adapter into the base model\n",
    "model = PeftModel.from_pretrained(model, '/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ପୂର୍ବ-ଗୋଟିଏ ଖୈଳଧଙ୍ଚ, ଯେଉଁଠି ଅଂଘାଡ଼ି ସୃଷ୍ଟି ଥିଲା। ଆଞ୍ଚଳିକ ସୌନ୍ଦର୍ଯ୍ୟ ସଫର କରିଛି, ଓ ସ୍ଥାନୀୟ ଔପନିବେଶକମାନେ ପ୍ରଵେଶ କରୁଛନ୍ତି।\n",
      "\" \" - ଢ. ପ୍ରୋଫେସର ଐ. ରେନି (1928) | ଏହା ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ସମ୍ପର୍କିତ ଏକ ଜାରିବାର ପାଳନ କରୁଛି । ରାଜାରାଣୀ ଦୁ\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved checkpoint\n",
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(save_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Restore model, optimizer, scheduler, and step\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "global_step = checkpoint['global_step']\n",
    "\n",
    "# print(f\"Checkpoint loaded from {save_path} at step {global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = checkpoint['global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'global_step': global_step\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Checkpoint saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
