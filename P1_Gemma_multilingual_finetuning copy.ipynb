{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 09:36:25.939805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745919385.949483 2141419 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745919385.952426 2141419 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 09:36:25.963251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda5fe6322b4a75bf248918fb1097c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3023.7419147491455\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoProcessor, BitsAndBytesConfig, get_scheduler,Gemma3ForConditionalGeneration\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import here opencv\n",
    "\n",
    "\n",
    "\n",
    "# device_name = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "# device = torch.device(device_name)\n",
    "\n",
    "DEFAULT_MODEL = \"google/gemma-3-4b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                                load_in_4bit=True,\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_storage=torch.bfloat16,\n",
    "                                )\n",
    "\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation='eager',\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    "    torch_dtype=torch.bfloat16\n",
    "    \n",
    ")\n",
    "\n",
    "print(model.get_memory_footprint()/(1024*1024)) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Multi Lingual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"OdiaGenAI/hardcode_odia_qa_105\")\n",
    "dataset = load_dataset('OdiaGenAI/odia_domain_context_train_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଓଡ଼ିଶାର ଉଦୟଗିରି ଏବଂ ଖଣ୍ଡଗିରି ଗୁମ୍ଫାର ଇତିହାସ କ’ଣ?',\n",
       " 'input': '',\n",
       " 'output': 'ଉଦୟଗିରି ଏବଂ ଖଣ୍ଡଗିରି ଗୁମ୍ଫା ଭାରତର ଓଡ଼ିଶା ରାଜ୍ୟରେ ଅବସ୍ଥିତ ପ୍ରାଚୀନ ଗୁମ୍ଫା ପରିସରର ଏକ ସମୂହ। ଏହି ଗୁମ୍ଫା ଗୁଡିକ ଖ୍ରୀଷ୍ଟପୂର୍ବ ଦ୍ୱିତୀୟ ଶତାବ୍ଦୀ ଏବଂ ପ୍ରଥମ ଶତାବ୍ଦୀ ମଧ୍ୟରେ ଜୈନ ସନ୍ନ୍ୟାସୀମାନଙ୍କ ଦ୍ୱାରା ବାଲୁକା ପଥରରେ ଖୋଦିତ ହୋଇଥିବା ଜଣାଯାଏ।\\nଉଦୟଗିରି ଗୁମ୍ଫା ଗୁଡିକ ବୃହତ ଏବଂ ଅଧିକ ଜଟିଳ ହୋଇଥିବାବେଳେ ଖଣ୍ଡଗିରି ଗୁମ୍ଫା ଗୁଡିକ ଛୋଟ ଏବଂ ଡିଜାଇନ ଦୃଷ୍ଟିରୁ ସରଳ। ଏହି ପ୍ରାଚୀନ ଗୁମ୍ଫା ଗୁଡିକ ପ୍ରାଚୀନ ଭାରତୀୟ ସଭ୍ୟତାର ସ୍ଥାପତ୍ୟ କୌଶଳ ଏବଂ କଳାତ୍ମକ ଶ୍ରେଷ୍ଠତାର ପ୍ରମାଣ।\\nବିଗତ ବର୍ଷମାନଙ୍କରେ ଏହି ଗୁମ୍ଫା ସାରା ବିଶ୍ୱରୁ ପର୍ଯ୍ୟଟକଙ୍କ ଆକର୍ଷଣ କେନ୍ଦ୍ର ପାଲଟିଛି ଏବଂ ଭାରତୀୟ ପ୍ରତ୍ନତାତ୍ୱିକ ସର୍ବେକ୍ଷଣ ଦ୍ୱାରା ସଂରକ୍ଷିତ ହୋଇଛି। ଭାରତର ସମୃଦ୍ଧ ସାଂସ୍କୃତିକ ଐତିହ୍ୟ ପ୍ରତି ଆଗ୍ରହୀ ଯେକୌଣସି ବ୍ୟକ୍ତି ଏହି ଗୁମ୍ଫାକୁ ଦେଖିବା ଉଚିତ।'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(tokenizer(example['instruction']+ example['output'])['input_ids']) for example in dataset['train']]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Tokenized Output Lengths\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Training using TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a5da1fc60a4d3bacb46500c1640faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      # {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": sample['instruction']},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=['output','instruction','input'],batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "# dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "# # Print formatted user prompt\n",
    "# print(dataset[\"train\"][345][\"messages\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nଓଡ଼ିଶାର ଅର୍ଥନୀତି ଉପରେ ପର୍ଯ୍ୟଟନର କିଭଳି ପ୍ରଭାବ ପଡ଼ିଛି?<end_of_turn>\\n<start_of_turn>model\\nବିଗତ କିଛି ବର୍ଷ ମଧ୍ୟରେ ପର୍ଯ୍ୟଟନ ଶିଳ୍ପ ଦ୍ୱାରା ଓଡ଼ିଶାର ଅର୍ଥନୀତି ବ୍ୟାପକ ଭାବେ ପ୍ରଭାବିତ ହୋଇଛି। ରାଜ୍ୟରେ ପ୍ରାକୃତିକ ସୌନ୍ଦର୍ଯ୍ୟ ଏବଂ ସାଂସ୍କୃତିକ ଐତିହ୍ୟ ସ୍ଥାନ ରହିଛି, ଯାହା ଦେଶ ଏବଂ ଦେଶ ବାହାରର ପର୍ଯ୍ୟଟକଙ୍କ ପାଇଁ ଆକର୍ଷଣୀୟ ହୋଇପାରିଛି।\\nପର୍ଯ୍ୟଟନ କ୍ଷେତ୍ରରେ କାର୍ଯ୍ୟ କରୁଥିବା ଲୋକମାନଙ୍କ ପାଇଁ ନିଯୁକ୍ତି ସୁଯୋଗ ଏବଂ ଆୟ ବୃଦ୍ଧି ପାଇଛି, ଯେଉଁଥିରେ ଆତିଥ୍ୟ, ଟୁର ଅପରେଟର ଏବଂ ପରିବହନ ଶିଳ୍ପ ସାମିଲ ରହିଛି। ପର୍ଯ୍ୟଟନ ଶିଳ୍ପ ସ୍ଥାନୀୟ ଖାଦ୍ୟ, ହସ୍ତଶିଳ୍ପ ଏବଂ ସାଂସ୍କୃତିକ ଗତିବିଧିକୁ ପ୍ରୋତ୍ସାହିତ କରିବାରେ ସହାୟତା କରିଛି, ଯାହାକି ଓଡ଼ିଶାର ସାମଗ୍ରିକ ଅର୍ଥନୀତି ଉପରେ ସକାରାତ୍ମକ ପ୍ରଭାବ ପକାଇଛି।\\nପର୍ଯ୍ୟଟକମାନଙ୍କ ପାଇଁ ଉତ୍ତମ ଅନୁଭୂତି ସୁନିଶ୍ଚିତ କରିବା ପାଇଁ ଭିତ୍ତିଭୂମି ବିକାଶ ଉପରେ ମଧ୍ୟ ସରକାର ଗୁରୁତ୍ୱ ଦେଉଛନ୍ତି। ସଡ଼କ ଏବଂ ସାର୍ବଜନୀନ ପରିବହନ ବ୍ୟବସ୍ଥାରେ ସୁଧାର କରାଯାଇଛି, ଯେତେବେଳେ କି ରାଜ୍ୟର ଅନେକ କ୍ଷେତ୍ରରେ ବିଭିନ୍ନ ଇକୋ-ଟୁରିଜମ ପ୍ରକଳ୍ପର ବିକାଶ କରାଯାଇଛି।\\nଶେଷରେ ପର୍ଯ୍ୟଟନ ଓଡ଼ିଶାର ଆର୍ଥିକ ବିକାଶକୁ ତ୍ୱରାନ୍ୱିତ କରିବାରେ ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ଭୂମିକା ନିର୍ବାହ କରିଛି ଏବଂ ରାଜ୍ୟବାସୀଙ୍କ ପାଇଁ ଅନେକ ଲାଭ ଆଣିପାରିଛି।<end_of_turn>\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(dataset['train'][0]['messages'],tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "torch_dtype = torch.bfloat16\n",
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-text-to-sql\",         # directory to save and repository id\n",
    "    max_seq_length=512,                     # max sequence length for model and packing of the dataset\n",
    "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2064' max='2064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2064/2064 1:02:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>5.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>5.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>4.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.782600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>4.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>4.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>4.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>5.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>4.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>4.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>4.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>4.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>4.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>4.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>4.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>4.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>4.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>4.872900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>4.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>4.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>4.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>4.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>4.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>4.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>3.925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>4.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>3.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>4.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>4.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>3.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>4.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>3.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>4.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>4.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>3.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>4.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>4.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>4.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>4.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>3.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>4.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>4.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>4.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>4.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>3.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>4.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>4.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>3.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>3.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>3.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>4.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>3.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>3.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>4.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>3.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>4.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>4.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>3.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>3.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>4.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>4.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>3.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>3.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>3.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>4.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>4.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>4.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>3.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>4.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>3.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>4.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>4.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>3.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>3.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>4.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>3.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>4.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>3.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>3.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>3.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>3.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>3.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>3.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>3.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>3.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>3.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>3.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>3.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>3.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>3.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>3.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>3.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>3.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>3.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>3.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>3.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>3.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>3.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>3.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>3.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>3.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>3.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>3.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>3.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>3.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>3.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>3.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>3.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>3.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>3.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>3.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>3.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>3.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>3.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>3.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>3.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>3.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>3.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>3.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>3.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>3.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>3.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>3.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>3.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>3.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>3.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>3.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>3.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>3.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>3.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>3.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>3.317100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/models/Mohan-diffuser/gemma-text-to-sql/preupload/main (Request ID: Root=1-680b835b-00f55311711422712ea6f078;3de44368-d484-46cb-9f99-ff2f9c551958)\n\nInvalid credentials in Authorization header",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/Mohan-diffuser/gemma-text-to-sql/preupload/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the final model again to the Hugging Face Hub\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/trainer.py:3906\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n\u001b[0;32m-> 3906\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel save\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/trainer.py:4841\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   4839\u001b[0m \u001b[38;5;66;03m# Wait for the current upload to be finished.\u001b[39;00m\n\u001b[1;32m   4840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish_current_push()\n\u001b[0;32m-> 4841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_as_future\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPREFIX_CHECKPOINT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1624\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4934\u001b[0m, in \u001b[0;36mHfApi.upload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[0m\n\u001b[1;32m   4930\u001b[0m commit_operations \u001b[38;5;241m=\u001b[39m delete_operations \u001b[38;5;241m+\u001b[39m add_operations\n\u001b[1;32m   4932\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m commit_message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload folder using huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4934\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4937\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4944\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4946\u001b[0m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[1;32m   4947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1624\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4193\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[1;32m   4191\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[0;32m-> 4193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4195\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[1;32m   4199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[1;32m   4202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4204\u001b[0m files_to_copy \u001b[38;5;241m=\u001b[39m _fetch_files_to_copy(\n\u001b[1;32m   4205\u001b[0m     copies\u001b[38;5;241m=\u001b[39mcopies,\n\u001b[1;32m   4206\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4210\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   4211\u001b[0m )\n\u001b[1;32m   4212\u001b[0m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4416\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4414\u001b[0m \u001b[38;5;66;03m# Check which new files are LFS\u001b[39;00m\n\u001b[1;32m   4415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4416\u001b[0m     \u001b[43m_fetch_upload_modes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4417\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_additions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgitignore_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgitignore_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4427\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:680\u001b[0m, in \u001b[0;36m_fetch_upload_modes\u001b[0;34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001b[0m\n\u001b[1;32m    672\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgitIgnore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gitignore_content\n\u001b[1;32m    674\u001b[0m resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/preupload/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    676\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    677\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    678\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_pr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    679\u001b[0m )\n\u001b[0;32m--> 680\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m preupload_info \u001b[38;5;241m=\u001b[39m _validate_preupload_info(resp\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    682\u001b[0m upload_modes\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]: file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muploadMode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m preupload_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/miniconda3/envs/buawei/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/Mohan-diffuser/gemma-text-to-sql/preupload/main (Request ID: Root=1-680b835b-00f55311711422712ea6f078;3de44368-d484-46cb-9f99-ff2f9c551958)\n\nInvalid credentials in Authorization header"
     ]
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f63dbd5dcf247d9b6343f2911d94b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1745581212.rack-gpu-02.1626713.0:   0%|          | 0.00/63.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0b2137ba2046c79cb58cae0d2136b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3790099a64e445d696fd0f2220578f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 13:05:25.680784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745586325.690528 1636331 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745586325.693544 1636331 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 13:05:25.704657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74954a30119e4219b426192a85124460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoProcessor, BitsAndBytesConfig, get_scheduler,Gemma3ForConditionalGeneration\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "model_id= \"google/gemma-3-4b-it\"\n",
    "# Load Model base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                                load_in_4bit=True,\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_quant_storage=torch.bfloat16,\n",
    "                                )\n",
    "\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation='eager',\n",
    "    device_map={'':torch.cuda.current_device()},\n",
    "    torch_dtype=torch.bfloat16\n",
    "    \n",
    ").eval()\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(model, 'gemma-text-to-sql').to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gemma-text-to-sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(model,idx=5,disable_lora=False):\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    sample=dataset['train'][idx]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to('cuda').to(torch.float16)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if disable_lora:\n",
    "        with model.disable_adapter():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=256,\n",
    "                repetition_penalty=1.3,\n",
    "                temperature=0.7,         # Optional: smooth randomness\n",
    "                top_k=50,                # Optional: top-k sampling\n",
    "                top_p=0.9                # Optional: nucleus sampling\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=256,\n",
    "        repetition_penalty=1.3,\n",
    "        temperature=0.2,         # Optional: smooth randomness\n",
    "        top_k=50,                # Optional: top-k sampling\n",
    "        top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mohan.dash/miniconda3/envs/buawei/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred,true=generate_eval(peft_model,idx=56,disable_lora=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "ପ୍ରତିବର୍ଷ ଓଡ଼ିଶା ପୁରସ୍କାର ବିଜେତାମାନଙ୍କୁ କିପରି ଚୟନ କରାଯାଇଥାଏ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ଓଡ଼ିଶା ସରକାର ପ୍ରତ୍ୟକ୍ଷ ଭାବରେ ଏହି ଉତ୍ସବ ଆୟୋଜନ କରିଥାନ୍ତି। ଅନ୍ୟ ରାଜ୍ୟଗୁଡିକଠାରୁ ଯୌନ ନିମ୍ନସ୍ତରୀୟ ଗତିବିଧିକୁ ଦୂର କରିବା ପାଇଁ ମଧ୍ୟ ଜଣେ ବ୍ୟକ୍ତିକୁ ଚୟନ କରିବା ପାଇଁ ଏହା ଏକ ପ୍ରଭାବଶାଳୀ ପ୍ରଦର୍ଷ୍ଟନୀ ହୋଇଛି, କିନ୍ତୁ ଏହାର ଶୈଳୀରେ ପରିବର୍ତ୍ତନ ଆସିଛି।\n",
      "ଅନ୍ୟ ଏକ ଖବରପତ୍ରରେ ଘଟିଥିବା ଅନୁଯାୟୀ, ପ୍ରତିବର୍ଷ ଓଡ଼ିଶା ସରକାର ‘ଓଡ଼ିଆ’ ରଙ୍ଗରେ ପାଞ୍ଚ ଜଣଙ୍କୁ ଚୟନ କରିଛନ୍ତି ଏବଂ ସେମାନଙ୍କୁ 100-250 ଲକ୍ଷ ଋଣ ପ୍ରଦାନ କରିଛନ୍ତି।\n",
      "ଓଡ଼ିଶା ପୁର\n",
      "******************************\n",
      "କ୍ରୀଡ଼ା, କଳା, ସାହିତ୍ୟ, ସଂସ୍କୃତି, ବିଜ୍ଞାନ ଓ ପ୍ରଯୁକ୍ତି ଏବଂ ସାମାଜିକ ସେବା ଭଳି ବିଭିନ୍ନ କ୍ଷେତ୍ରରେ ଉଲ୍ଲେଖନୀୟ ସଫଳତା, ନବୋନ୍ମେଷ ଏବଂ ଉତ୍କର୍ଷକୁ ସ୍ୱୀକୃତି ପ୍ରଦାନ କରିବା ପାଇଁ ଏହି ପୁରସ୍କାର ପ୍ରଦାନ କରାଯାଇଥାଏ।\n",
      "ଚୟନ ପ୍ରକ୍ରିୟାରେ ଅନେକ ପର୍ଯ୍ୟାୟରେ ନାମାଙ୍କନ ଆହ୍ୱାନ କରାଯାଇଥାଏ, ଯାହା ପରେ ଯୋଗ୍ୟ ପ୍ରାର୍ଥୀମାନଙ୍କୁ ନିର୍ଦ୍ଦିଷ୍ଟ ମାନଦଣ୍ଡ ଏବଂ ମାନଦଣ୍ଡ ଆଧାରରେ ଚୟନ କରାଯାଇଥାଏ। ଏହାପରେ ଚୟନ କରାଯାଇଥିବା ପ୍ରାର୍ଥୀମାନଙ୍କୁ ବିଚାରପତିମାନଙ୍କ ଏକ ପ୍ୟାନେଲ ଦ୍ୱାରା ମୂଲ୍ୟାଙ୍କନ କରାଯାଇଥାଏ, ଯେଉଁମାନେ ପ୍ରତ୍ୟେକ ପ୍ରାର୍ଥୀଙ୍କ ପ୍ରୋଫାଇଲ, ଉପଲବ୍ଧି ଏବଂ ସେମାନଙ୍କ ନିଜ ନିଜ କ୍ଷେତ୍ରରେ ଯୋଗଦାନ ବିଷୟରେ ଧ୍ୟାନପୂର୍ବକ ସମୀକ୍ଷା କରିଥାନ୍ତି।\n",
      "ଏଠାରେ ଉଲ୍ଲେଖନୀୟ ଯେ ଚୟନ ପ୍ରକ୍ରିୟା ସ୍ୱଚ୍ଛ ଏବଂ ନିରପେକ୍ଷ ଏବଂ ବିଚାରପତିମାନଙ୍କ ପ୍ୟାନେଲ ନିଜ ନିଜ କ୍ଷେତ୍ରରେ ପ୍ରତିଷ୍ଠିତ ଏବଂ ପ୍ରତିଷ୍ଠିତ ବ୍ୟକ୍ତିବିଶେଷଙ୍କୁ ନେଇ ଗଠିତ ହୋଇଥାଏ। ଏହା ସୁନିଶ୍ଚିତ କରିଥାଏ ଯେ ପୁରସ୍କାର ଉତ୍କର୍ଷ ଏବଂ ଯୋଗ୍ଯତାର ପ୍ରକୃତ ପ୍ରତିଫଳନ ଏବଂ ବିଜେତାମାନେ ବାସ୍ତବରେ ମାନ୍ୟତା ଏବଂ ପ୍ରଶଂସାର ହକଦାର।\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print('*'*30)\n",
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Pytorch Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"what is capital of france?\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "# outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user\\nYou are a helpful assistant\\n\\nwhat is capital of france?<end_of_turn>\\n<start_of_turn>model\\nThe capital of France is **Paris**. 😊 \\n\\nIt’s a beautiful and iconic city! \\n\\nIs there anything else you’d like to know about France or Paris?<end_of_turn>']\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Object for Pytorch-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "from os import truncate\n",
    "\n",
    "\n",
    "class LlamaDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        question=sample['instruction']\n",
    "        answer = sample['output']\n",
    "        prompt = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "        full_text = prompt+f'''{answer}<end_of_turn>'''\n",
    "\n",
    "        tokenized = tokenizer(full_text,add_special_tokens=False,truncation=True,max_length=250)\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        # Tokenize just the prompt to get the split point\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        answer_start = len(prompt_ids)\n",
    "\n",
    "        # Mask everything before answer_start\n",
    "        labels = [-100] * answer_start + input_ids[answer_start:]\n",
    "        # Mask out padding as well\n",
    "        labels = [\n",
    "            label if token != tokenizer.pad_token_id else -100\n",
    "            for label, token in zip(labels, input_ids)\n",
    "        ]\n",
    "    \n",
    "        return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def llama_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the max length in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": labels_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Assume dataset['train'] is the full dataset you want to split\n",
    "full_dataset = dataset['train']\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = random_split(full_dataset, [0.95, 0.05])\n",
    "\n",
    "# Wrap in your custom Dataset class\n",
    "train_dataset = LlamaDataset(train_data)\n",
    "test_dataset = LlamaDataset(test_data)\n",
    "\n",
    "# DataLoaders with custom collate_fn\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=llama_collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=llama_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 98])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower.vision_model.embeddings.patch_embedding.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.embeddings.patch_embedding.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.embeddings.position_embedding.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.post_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.post_layernorm.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "multi_modal_projector.mm_input_projection_weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "multi_modal_projector.mm_soft_emb_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.16.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.17.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.18.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.19.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.20.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.21.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.22.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.23.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.24.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.25.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.26.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.27.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.28.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.29.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.30.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.31.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.32.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "language_model.model.layers.33.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of `prepare_model_for_kbit_training`\n",
    "\n",
    "When you load a model in 4-bit or 8-bit precision using bitsandbytes, some layers (like LayerNorm) still remain in full precision (float32), and certain operations (like weight updates) can be unstable or incompatible if done blindly on quantized weights.\n",
    "\n",
    "- Casts `LayerNorm` layers to `float32`\n",
    "- Sets `requires_grad=False` for all model parameters\n",
    "- Wraps the output layer (like `lm_head`) in `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 42,734,080 || all params: 4,342,813,552 || trainable%: 0.9840\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vision_tower.vision_model.embeddings.patch_embedding.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.embeddings.patch_embedding.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.embeddings.position_embedding.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.post_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.post_layernorm.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.multi_modal_projector.mm_input_projection_weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.multi_modal_projector.mm_soft_emb_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.lm_head.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.lm_head.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = '''ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର କିପରି ମିଳିମିଶି କାର୍ଯ୍ୟ କରିପାରିବେ?'''\n",
    "# answer = '''ଯେକୌଣସି ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ମିଳିତ ପ୍ରୟାସର ଆବଶ୍ୟକତା ରହିଛି। ଓଡ଼ିଶାର ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ବିସ୍ତୃତ ରଣନୀତି ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିବା ଲାଗି ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିପାରିବେ।\n",
    "# ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହିତ ମିଶି କାମ କରିବାର ଗୋଟିଏ ଉପାୟ ହେଲା ଘରୋଇ ନିବେଶ ପାଇଁ ଅନୁକୂଳ ବାତାବରଣ ସୃଷ୍ଟି କରିବା, ଏଥିରେ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ଟିକସ ଏବଂ ନିୟାମକ ପ୍ରତିବନ୍ଧକକୁ ହ୍ରାସ କରିବା, ଏହା ବ୍ୟତୀତ ସରକାର ଟିକସ ରିହାତି, ସବସିଡି ଏବଂ ପର୍ଯ୍ୟଟନ ବିକାଶ ପ୍ରକଳ୍ପ ପାଇଁ ଜମି ଆଦି ପ୍ରୋତ୍ସାହନ ମଧ୍ୟ ପ୍ରଦାନ କରିପାରିବେ।\n",
    "# ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ନୂତନ ପର୍ଯ୍ୟଟନ ଉତ୍ପାଦ ପ୍ରସ୍ତୁତ କରିପାରିବେ ଯାହା ଉଭୟ ଘରୋଇ ଏବଂ ଅନ୍ତର୍ଜାତୀୟ ପର୍ଯ୍ୟଟକଙ୍କ ଆବଶ୍ୟକତା ପୂରଣ କରିପାରିବ।\n",
    "# ସରକାର ମଧ୍ୟ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ପ୍ରଯୁକ୍ତିର ଉପଯୋଗ କରିପାରିବେ। ଉଦାହରଣ ସ୍ୱରୂପ, ସରକାର ପର୍ଯ୍ୟଟନ ସ୍ଥଳକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ଏବଂ ସମ୍ଭାବ୍ୟ ପର୍ଯ୍ୟଟକମାନଙ୍କ ସହିତ ଯୋଡ଼ିବା ଲାଗି ସୋସିଆଲ ମିଡିଆ ପ୍ଲାଟଫର୍ମର ଉପଯୋଗ କରିପାରିବେ। ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନ ଆକର୍ଷଣ ଏବଂ ଅନୁଭବ ପ୍ରଦର୍ଶିତ କରିବା ଲାଗି ଏକ ଅନଲାଇନ ପ୍ଲାଟଫର୍ମ ପ୍ରତିଷ୍ଠା କରିବା ଦ୍ୱାରା ଅଧିକ ପର୍ଯ୍ୟଟକଙ୍କୁ ଆକର୍ଷିତ କରିବାରେ ସହାୟତା ମିଳିପାରିବ।\n",
    "# ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିବାକୁ ପଡିବ ଯେପରିକି ପର୍ଯ୍ୟଟନ ଗତିବିଧି ଦ୍ୱାରା ପର୍ଯ୍ୟାବରଣର କ୍ଷୟ କିମ୍ବା ସ୍ଥାନୀୟ ସମ୍ପ୍ରଦାୟର କ୍ଷତି ନ ହେଉ।\n",
    "# ଶେଷରେ, ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ଅନୁକୂଳ ପରିବେଶ ସୃଷ୍ଟି କରିବା ଆବଶ୍ୟକ ଏବଂ ଏହା ସୁନିଶ୍ଚିତ କରିବା ଉଚିତ ଯେ ବିକାଶ ସ୍ଥାୟୀ ହେବ। ” ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରି ସେମାନେ ପର୍ଯ୍ୟଟନ ରଣନୀତିକୁ ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିପାରିବେ ଯାହା କେବଳ ପର୍ଯ୍ୟଟନ ଉଦ୍ୟୋଗ ନୁହେଁ ବରଂ ସ୍ଥାନୀୟ ଗୋଷ୍ଠୀ ଏବଂ ପରିବେଶକୁ ମଧ୍ୟ ଲାଭାନ୍ୱିତ କରିବ।'''\n",
    "\n",
    "# tokenized_text = tokenizer(answer).input_ids\n",
    "# print(len(tokenized_text))\n",
    "# for idx in range(len(tokenized_text)):\n",
    "#     clear_output(wait=True)\n",
    "#     print(tokenizer.decode(tokenized_text[0:idx]))\n",
    "#     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the LLAMA model on a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"user\\nବିଶାଳ ଶିଶ୍ନକୁ ଚୁମ୍ବନ କରିଥିବାରୁ ମହିଳାମାନଙ୍କୁ କ 'ଣ ପୁରସ୍କାର ଦିଆଯାଉଛି?\\nmodel\\nପ୍ରସିଦ୍ଧ ସ୍ଥାନୀୟ ଟିସିପୋରୋ ମଦ ସ୍ପିରିଟର ଏକ ଦୃଶ୍ୟ\",\n",
       " 'user\\nଏପରିକି ପଇସା ପ୍ରବାହକୁ ନିୟନ୍ତ୍ରଣରେ ରଖିବାରେ କିପରି ସାହାଯ୍ୟ କରିବ?\\nmodel\\nଦୁର୍ବଳ ସମ୍ପତ୍ତିଗୁଡ଼ିକୁ କିଣିବା ସମୟରେ ଉଚ୍ଚ ମୂଲ୍ୟବାନ ସମ୍ପତ୍ତିଗୁଡ଼ିକୁ ବିକ୍ରି କରାଯାଇଥାଏ।']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "tokenizer.batch_decode(batch['input_ids'],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(model,idx=5,disable_lora=False):\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    sample=dataset['train'][idx]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to('cuda').to(torch.float16)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if disable_lora:\n",
    "        with model.disable_adapter():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=256,\n",
    "                repetition_penalty=1.3,\n",
    "                temperature=0.7,         # Optional: smooth randomness\n",
    "                top_k=50,                # Optional: top-k sampling\n",
    "                top_p=0.9                # Optional: nucleus sampling\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=256,\n",
    "        repetition_penalty=1.3,\n",
    "        temperature=0.7,         # Optional: smooth randomness\n",
    "        top_k=50,                # Optional: top-k sampling\n",
    "        top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?',\n",
       " 'input': '',\n",
       " 'output': 'ଏକାଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମାଣ କରାଯାଇଥିବା ରାଜାରାଣୀ ମନ୍ଦିର ଭୁବନେଶ୍ୱର ସହରର ଏକ ଲୋକପ୍ରିୟ ପର୍ଯ୍ୟଟନସ୍ଥଳୀ।\\nଏକାଦଶ ଶତାବ୍ଦୀରେ ଏହି ଅଂଚଳରେ ରାଜାରାଣୀ ରାଜବଂଶ ଶାସନ କରିଥିଲେ। ବିଶ୍ୱାସ କରାଯାଏ ଯେ ଏହି ମନ୍ଦିର ପ୍ରଥମେ ଭଗବାନ ଶିବଙ୍କୁ ସମର୍ପିତ ଥିଲା, କିନ୍ତୁ ପରେ ଏହା ଭଗବାନ ବିଷ୍ଣୁଙ୍କ ମନ୍ଦିର ପାଲଟିଥିଲା।\\nରାଜାରାଣୀ ମନ୍ଦିରର ସ୍ଥାପତ୍ୟ ଅଦ୍ୱିତୀୟ, କାରଣ ଏଠାରେ କୌଣସି ପବିତ୍ର ସ୍ଥାନ କିମ୍ବା କେନ୍ଦ୍ରୀୟ ମନ୍ଦିର ନାହିଁ, ଯାହାକି ହିନ୍ଦୁ ମନ୍ଦିରର ଏକ ବିଶେଷ ବୈଶିଷ୍ଟ୍ୟ, ଏହା ପରିବର୍ତ୍ତେ ମନ୍ଦିରର ଚାରିପଟେ ଛୋଟ ଛୋଟ ମନ୍ଦିର ରହିଛି ଯେଉଁଥିରେ ବିଭିନ୍ନ ଦେବଦେବୀଙ୍କ ମୂର୍ତ୍ତି ରହିଛି।\\nଏହି ମନ୍ଦିରର ଅନ୍ୟ ଏକ ବିଶେଷତ୍ୱ ହେଉଛି ଏହାର ସୁସଜ୍ଜିତ ଭାସ୍କର୍ଯ୍ୟ ଏବଂ ଖୋଦାଇ କାର୍ଯ୍ୟ। ଏହି ମନ୍ଦିରରେ ଦେବ-ଦେବୀ ଏବଂ ପୌରାଣିକ ପ୍ରାଣୀମାନଙ୍କ ଜଟିଳ ଚିତ୍ରଣ ରହିଛି, ଯାହା ଏହାକୁ କଳା ଏବଂ ସ୍ଥାପତ୍ୟର ଉତ୍ସାହୀମାନଙ୍କ ପାଇଁ ଏକ ଲୋକପ୍ରିୟ ଗନ୍ତବ୍ୟ ସ୍ଥଳୀରେ ପରିଣତ କରିଛି।\\nବିଗତ ବର୍ଷମାନଙ୍କରେ, ରାଜାରାଣୀ ମନ୍ଦିର ଭୁବନେଶ୍ୱରର ଏକ ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ସାଂସ୍କୃତିକ ସ୍ଥଳୀରେ ପରିଣତ ହୋଇଛି ଏବଂ ଏହି କ୍ଷେତ୍ରର ସମୃଦ୍ଧ ଇତିହାସ ଏବଂ ସାଂସ୍କୃତିକ ଐତିହ୍ୟର ଏକ ଉଦାହରଣ।'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ଭୁବନ୍ଏଷ୍ୱରର ରୋjería ବିଳସ୍ଵମାନିକ୍ର ସଦୟା ପୂର୍ଣ୍ଣ ପ୍ରକୃତ ଦୈବାଲ୍ୟ ମନ୍ଦିର, ଜାହ଼ିଲା ଭୁବନ୍ ଏହି ଅଞ୍ଚଳରେ ଗୌରВО ବଗିକ ଓ ଶିଆଙ୍କ religiónର ଯୋଗଯାତା। ଏଠାରୁ ଆପ negated ବିଲସ୍ଵମାନିକ୍ର ମନ୍ଦିର ନାም።\n",
      "\n",
      "**ଇତିହାସ:**\n",
      "*   ମନ୍ଦିରଟିକୁ 12-ମ୍ centenary веке ସ୍କ୍ରଓଡ଼ା ହରିହାରାଙ୍ género ସହିଁ строительства କରିଥିଲେ ।\n",
      "*   ମନ୍ଦିରଟିକୁ 16वीं century में ବିସ୍ଵମାନିକ୍ରଙ୍କ ସହିଁ строительство କରିଥିଲେ ।\n",
      "*   ଉତ୍ତରାଧ୍ୟକ୍ଷ ସଂଘ ବାଳକ୍ର ମାଝିНЫ କରିଛନ୍ତି , ଯେ ମନ୍ଦିରଟିକୁ 830 CE তে построен थी और 14 वीं शताब्दी तक यह अस्तित्व में रहती है।\n",
      "*   ମନ୍ଦିରଟି ସଦୟା ସନ୍ତ୍ରଷ୍ଟ\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Loss: 3.2277\n",
      "Step 2/1000, Loss: 3.8620\n",
      "Step 3/1000, Loss: 3.2419\n",
      "Step 4/1000, Loss: 2.9617\n",
      "Step 5/1000, Loss: 2.9862\n",
      "Step 6/1000, Loss: 3.3593\n",
      "Step 7/1000, Loss: 1.8443\n",
      "Step 8/1000, Loss: 2.3608\n",
      "Step 9/1000, Loss: 3.8640\n",
      "Step 10/1000, Loss: 2.3987\n",
      "Step 11/1000, Loss: 1.9696\n",
      "Step 12/1000, Loss: 2.3811\n",
      "Step 13/1000, Loss: 2.8833\n",
      "Step 14/1000, Loss: 2.3822\n",
      "Step 15/1000, Loss: 2.6129\n",
      "Step 16/1000, Loss: 2.6784\n",
      "Step 17/1000, Loss: 1.5637\n",
      "Step 18/1000, Loss: 1.6347\n",
      "Step 19/1000, Loss: 2.3786\n",
      "Step 20/1000, Loss: 2.0025\n",
      "Step 21/1000, Loss: 3.0606\n",
      "Step 22/1000, Loss: 2.1785\n",
      "Step 23/1000, Loss: 8.5814\n",
      "Step 24/1000, Loss: 1.7926\n",
      "Step 25/1000, Loss: 3.3365\n",
      "Step 26/1000, Loss: 2.6615\n",
      "Step 27/1000, Loss: 2.4189\n",
      "Step 28/1000, Loss: 1.6787\n",
      "Step 29/1000, Loss: 1.9527\n",
      "Step 30/1000, Loss: 2.7052\n",
      "Step 31/1000, Loss: 2.7806\n",
      "Step 32/1000, Loss: 2.0412\n",
      "Step 33/1000, Loss: 2.8274\n",
      "Step 34/1000, Loss: 1.8963\n",
      "Step 35/1000, Loss: 2.7080\n",
      "Step 36/1000, Loss: 2.8834\n",
      "Step 37/1000, Loss: 2.5970\n",
      "Step 38/1000, Loss: 2.4223\n",
      "Step 39/1000, Loss: 2.4469\n",
      "Step 40/1000, Loss: 2.1359\n",
      "Step 41/1000, Loss: 2.4251\n",
      "Step 42/1000, Loss: 2.0823\n",
      "Step 43/1000, Loss: 2.4795\n",
      "Step 44/1000, Loss: 2.6349\n",
      "Step 45/1000, Loss: 2.9278\n",
      "Step 46/1000, Loss: 1.9535\n",
      "Step 47/1000, Loss: 2.3210\n",
      "Step 48/1000, Loss: 1.9462\n",
      "Step 49/1000, Loss: 2.2596\n",
      "Step 50/1000, Loss: 2.3144\n",
      "Step 51/1000, Loss: 2.4391\n",
      "Step 52/1000, Loss: 1.6854\n",
      "Step 53/1000, Loss: 6.9138\n",
      "Step 54/1000, Loss: 2.1819\n",
      "Step 55/1000, Loss: 1.9856\n",
      "Step 56/1000, Loss: 0.9475\n",
      "Step 57/1000, Loss: 3.2484\n",
      "Step 58/1000, Loss: 23.5129\n",
      "Step 59/1000, Loss: 1.9959\n",
      "Step 60/1000, Loss: 4.4378\n",
      "Step 61/1000, Loss: 2.5346\n",
      "Step 62/1000, Loss: 2.8985\n",
      "Step 63/1000, Loss: 2.0992\n",
      "Step 64/1000, Loss: 3.3569\n",
      "Step 65/1000, Loss: 2.4521\n",
      "Step 66/1000, Loss: 2.9692\n",
      "Step 67/1000, Loss: 2.3598\n",
      "Step 68/1000, Loss: 2.1929\n",
      "Step 69/1000, Loss: 2.2840\n",
      "Step 70/1000, Loss: 2.6961\n",
      "Step 71/1000, Loss: 1.9702\n",
      "Step 72/1000, Loss: 2.5580\n",
      "Step 73/1000, Loss: 2.1248\n",
      "Step 74/1000, Loss: 2.4012\n",
      "Step 75/1000, Loss: 2.9509\n",
      "Step 76/1000, Loss: 2.1680\n",
      "Step 77/1000, Loss: 2.9800\n",
      "Step 78/1000, Loss: 1.7596\n",
      "Step 79/1000, Loss: 2.4966\n",
      "Step 80/1000, Loss: 2.2292\n",
      "Step 81/1000, Loss: 2.0234\n",
      "Step 82/1000, Loss: 2.3912\n",
      "Step 83/1000, Loss: 2.3201\n",
      "Step 84/1000, Loss: 4.7291\n",
      "Step 85/1000, Loss: 2.2465\n",
      "Step 86/1000, Loss: 1.7985\n",
      "Step 87/1000, Loss: 2.1821\n",
      "Step 88/1000, Loss: 1.7711\n",
      "Step 89/1000, Loss: 1.8232\n",
      "Step 90/1000, Loss: 41.1913\n",
      "Step 91/1000, Loss: 25.8548\n",
      "Step 92/1000, Loss: 2.4345\n",
      "Step 93/1000, Loss: 2.8527\n",
      "Step 94/1000, Loss: 3.9506\n",
      "Step 95/1000, Loss: 3.1605\n",
      "Step 96/1000, Loss: 164.3668\n",
      "Step 97/1000, Loss: 1.8823\n",
      "Step 98/1000, Loss: 3.1577\n",
      "Step 99/1000, Loss: 2.5394\n",
      "Step 100/1000, Loss: 3.1558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306c95892ce841d1ad1c7ecc1851a8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval at step 100] Val Loss: 3.4642\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 101/1000, Loss: 2.9044\n",
      "Step 102/1000, Loss: 3.1285\n",
      "Step 103/1000, Loss: 2.4170\n",
      "Step 104/1000, Loss: 2.4910\n",
      "Step 105/1000, Loss: 2.8710\n",
      "Step 106/1000, Loss: 3.1304\n",
      "Step 107/1000, Loss: 2.6397\n",
      "Step 108/1000, Loss: 2.3114\n",
      "Step 109/1000, Loss: 2.4866\n",
      "Step 110/1000, Loss: 2.7839\n",
      "Step 111/1000, Loss: 2.3212\n",
      "Step 112/1000, Loss: 2.0915\n",
      "Step 113/1000, Loss: 35.9083\n",
      "Step 114/1000, Loss: 2.5053\n",
      "Step 115/1000, Loss: 2.1414\n",
      "Step 116/1000, Loss: 2.9648\n",
      "Step 117/1000, Loss: 153.2257\n",
      "Step 118/1000, Loss: 122.5703\n",
      "Step 119/1000, Loss: 6.9447\n",
      "Step 120/1000, Loss: 5.1345\n",
      "Step 121/1000, Loss: 5.7772\n",
      "Step 122/1000, Loss: 5.7445\n",
      "Step 123/1000, Loss: 5.4187\n",
      "Step 124/1000, Loss: 5.0302\n",
      "Step 125/1000, Loss: 4.7209\n",
      "Step 126/1000, Loss: 4.5081\n",
      "Step 127/1000, Loss: 4.8164\n",
      "Step 128/1000, Loss: 4.6811\n",
      "Step 129/1000, Loss: 4.5569\n",
      "Step 130/1000, Loss: 4.6392\n",
      "Step 131/1000, Loss: 4.6987\n",
      "Step 132/1000, Loss: 4.8867\n",
      "Step 133/1000, Loss: 4.6533\n",
      "Step 134/1000, Loss: 4.5328\n",
      "Step 135/1000, Loss: 4.3220\n",
      "Step 136/1000, Loss: 4.7285\n",
      "Step 137/1000, Loss: 4.5423\n",
      "Step 138/1000, Loss: 4.3566\n",
      "Step 139/1000, Loss: 4.2074\n",
      "Step 140/1000, Loss: 4.3752\n",
      "Step 141/1000, Loss: 4.3607\n",
      "Step 142/1000, Loss: 4.4886\n",
      "Step 143/1000, Loss: 4.4473\n",
      "Step 144/1000, Loss: 4.3459\n",
      "Step 145/1000, Loss: 4.4156\n",
      "Step 146/1000, Loss: 4.6740\n",
      "Step 147/1000, Loss: 4.2921\n",
      "Step 148/1000, Loss: 4.3289\n",
      "Step 149/1000, Loss: 4.6500\n",
      "Step 150/1000, Loss: 4.4198\n",
      "Step 151/1000, Loss: 4.2036\n",
      "Step 152/1000, Loss: 4.5110\n",
      "Step 153/1000, Loss: 4.1869\n",
      "Step 154/1000, Loss: 4.0953\n",
      "Step 155/1000, Loss: 4.2587\n",
      "Step 156/1000, Loss: 4.4498\n",
      "Step 157/1000, Loss: 4.3839\n",
      "Step 158/1000, Loss: 4.3214\n",
      "Step 159/1000, Loss: 4.2297\n",
      "Step 160/1000, Loss: 4.3289\n",
      "Step 161/1000, Loss: 5.2878\n",
      "Step 162/1000, Loss: 4.2187\n",
      "Step 163/1000, Loss: 3.8540\n",
      "Step 164/1000, Loss: 4.1477\n",
      "Step 165/1000, Loss: 4.3188\n",
      "Step 166/1000, Loss: 4.0508\n",
      "Step 167/1000, Loss: 4.4276\n",
      "Step 168/1000, Loss: 4.2117\n",
      "Step 169/1000, Loss: 5.7749\n",
      "Step 170/1000, Loss: 4.1348\n",
      "Step 171/1000, Loss: 4.0301\n",
      "Step 172/1000, Loss: 4.5451\n",
      "Step 173/1000, Loss: 4.1228\n",
      "Step 174/1000, Loss: 4.2467\n",
      "Step 175/1000, Loss: 4.8889\n",
      "Step 176/1000, Loss: 4.3537\n",
      "Step 177/1000, Loss: 4.2130\n",
      "Step 178/1000, Loss: 4.3147\n",
      "Step 179/1000, Loss: 4.0425\n",
      "Step 180/1000, Loss: 4.0357\n",
      "Step 181/1000, Loss: 4.2483\n",
      "Step 182/1000, Loss: 4.0110\n",
      "Step 183/1000, Loss: 4.1835\n",
      "Step 184/1000, Loss: 3.6755\n",
      "Step 185/1000, Loss: 4.3799\n",
      "Step 186/1000, Loss: 14.6698\n",
      "Step 187/1000, Loss: 3.7327\n",
      "Step 188/1000, Loss: 4.3491\n",
      "Step 189/1000, Loss: 4.2711\n",
      "Step 190/1000, Loss: 4.0263\n",
      "Step 191/1000, Loss: 4.2074\n",
      "Step 192/1000, Loss: 3.7875\n",
      "Step 193/1000, Loss: 26.3267\n",
      "Step 194/1000, Loss: 4.4198\n",
      "Step 195/1000, Loss: 4.1929\n",
      "Step 196/1000, Loss: 4.1826\n",
      "Step 197/1000, Loss: 4.0103\n",
      "Step 198/1000, Loss: 3.9889\n",
      "Step 199/1000, Loss: 5.1423\n",
      "Step 200/1000, Loss: 4.2646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adff5d7cada541bc86d03ed841494689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval at step 200] Val Loss: 5.5605\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 201/1000, Loss: 4.3400\n",
      "Step 202/1000, Loss: 4.2810\n",
      "Step 203/1000, Loss: 4.3724\n",
      "Step 204/1000, Loss: 4.2403\n",
      "Step 205/1000, Loss: 4.2529\n",
      "Step 206/1000, Loss: 4.3290\n",
      "Step 207/1000, Loss: 4.4955\n",
      "Step 208/1000, Loss: 4.3536\n",
      "Step 209/1000, Loss: 4.3251\n",
      "Step 210/1000, Loss: 67.8426\n",
      "Step 211/1000, Loss: 4.5665\n",
      "Step 212/1000, Loss: 4.3994\n",
      "Step 213/1000, Loss: 4.3559\n",
      "Step 214/1000, Loss: 4.4021\n",
      "Step 215/1000, Loss: 4.4337\n",
      "Step 216/1000, Loss: 26.6376\n",
      "Step 217/1000, Loss: 4.6364\n",
      "Step 218/1000, Loss: 4.4278\n",
      "Step 219/1000, Loss: 4.4374\n",
      "Step 220/1000, Loss: 4.4348\n",
      "Step 221/1000, Loss: 4.5824\n",
      "Step 222/1000, Loss: 4.5504\n",
      "Step 223/1000, Loss: 4.5483\n",
      "Step 224/1000, Loss: 4.4290\n",
      "Step 225/1000, Loss: 4.2394\n",
      "Step 226/1000, Loss: 4.2976\n",
      "Step 227/1000, Loss: 4.3568\n",
      "Step 228/1000, Loss: 4.3794\n",
      "Step 229/1000, Loss: 4.3986\n",
      "Step 230/1000, Loss: 4.3433\n",
      "Step 231/1000, Loss: 4.5907\n",
      "Step 232/1000, Loss: 45.9385\n",
      "Step 233/1000, Loss: 4.3157\n",
      "Step 234/1000, Loss: 4.4138\n",
      "Step 235/1000, Loss: 5.6666\n",
      "Step 236/1000, Loss: 4.3997\n",
      "Step 237/1000, Loss: 4.3229\n",
      "Step 238/1000, Loss: 5.1326\n",
      "Step 239/1000, Loss: 4.8217\n",
      "Step 240/1000, Loss: 4.4225\n",
      "Step 241/1000, Loss: 4.4651\n",
      "Step 242/1000, Loss: 4.5790\n",
      "Step 243/1000, Loss: 4.4287\n",
      "Step 244/1000, Loss: 4.1085\n",
      "Step 245/1000, Loss: 4.4183\n",
      "Step 246/1000, Loss: 4.2673\n",
      "Step 247/1000, Loss: 3.9764\n",
      "Step 248/1000, Loss: 4.4617\n",
      "Step 249/1000, Loss: 4.3394\n",
      "Step 250/1000, Loss: 4.1583\n",
      "Step 251/1000, Loss: 4.2965\n",
      "Step 252/1000, Loss: 4.4072\n",
      "Step 253/1000, Loss: 4.1137\n",
      "Step 254/1000, Loss: 4.1598\n",
      "Step 255/1000, Loss: 4.1095\n",
      "Step 256/1000, Loss: 4.1615\n",
      "Step 257/1000, Loss: 13.8529\n",
      "Step 258/1000, Loss: 4.1427\n",
      "Step 259/1000, Loss: 4.3806\n",
      "Step 260/1000, Loss: 4.2409\n",
      "Step 261/1000, Loss: 4.0145\n",
      "Step 262/1000, Loss: 4.1612\n",
      "Step 263/1000, Loss: 6.8764\n",
      "Step 264/1000, Loss: 5.5849\n",
      "Step 265/1000, Loss: 4.2219\n",
      "Step 266/1000, Loss: 4.3722\n",
      "Step 267/1000, Loss: 4.2423\n",
      "Step 268/1000, Loss: 4.1012\n",
      "Step 269/1000, Loss: 4.1071\n",
      "Step 270/1000, Loss: 8.3597\n",
      "Step 271/1000, Loss: 4.0963\n",
      "Step 272/1000, Loss: 4.1724\n",
      "Step 273/1000, Loss: 4.1437\n",
      "Step 274/1000, Loss: 23.7644\n",
      "Step 275/1000, Loss: 4.1845\n",
      "Step 276/1000, Loss: 4.3000\n",
      "Step 277/1000, Loss: 4.1075\n",
      "Step 278/1000, Loss: 4.2243\n",
      "Step 279/1000, Loss: 4.6705\n",
      "Step 280/1000, Loss: 4.5045\n",
      "Step 281/1000, Loss: 4.3313\n",
      "Step 282/1000, Loss: 4.2095\n",
      "Step 283/1000, Loss: 4.7262\n",
      "Step 284/1000, Loss: 4.0221\n",
      "Step 285/1000, Loss: 4.2125\n",
      "Step 286/1000, Loss: 4.2672\n",
      "Step 287/1000, Loss: 4.0880\n",
      "Step 288/1000, Loss: 68.2998\n",
      "Step 289/1000, Loss: 7.3493\n",
      "Step 290/1000, Loss: 4.0698\n",
      "Step 291/1000, Loss: 4.0886\n",
      "Step 292/1000, Loss: 236.5284\n",
      "Step 293/1000, Loss: 4.1967\n",
      "Step 294/1000, Loss: 27.9700\n",
      "Step 295/1000, Loss: 13.8060\n",
      "Step 296/1000, Loss: 6.6630\n",
      "Step 297/1000, Loss: 8.5703\n",
      "Step 298/1000, Loss: 5.4950\n",
      "Step 299/1000, Loss: 5.2746\n",
      "Step 300/1000, Loss: 5.0039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b70fbc458be4a5194d8cde66a69fb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(test_dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_dataloader), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 69\u001b[0m     val_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_outputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     75\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:1326\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mignore_index, labels)\n\u001b[1;32m   1323\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_causal_mask(\n\u001b[1;32m   1324\u001b[0m     attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n\u001b[1;32m   1325\u001b[0m )\n\u001b[0;32m-> 1326\u001b[0m outputs: CausalLMOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m   1340\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:942\u001b[0m, in \u001b[0;36mGemma3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    939\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    940\u001b[0m )\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:722\u001b[0m, in \u001b[0;36mGemma3TextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    709\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    710\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m         last_cache_position,\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_cache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:420\u001b[0m, in \u001b[0;36mGemma3DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m position_embeddings_global\n\u001b[0;32m--> 420\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    432\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:322\u001b[0m, in \u001b[0;36mGemma3Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin,\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos,\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,\n\u001b[1;32m    321\u001b[0m     }\n\u001b[0;32m--> 322\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# Here we need to slice as we use a static cache by default, but FA2 does not support it\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/cache_utils.py:1768\u001b[0m, in \u001b[0;36mHybridCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# These two `if` blocks are only reached in multigpu and if `layer_device_map` is not passed. They are used\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# when the cache is initialized in the forward pass (e.g. Gemma2)\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx]\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m-> 1768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\u001b[38;5;241m.\u001b[39mto(value_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "gradient_accumulation_steps = 4\n",
    "max_steps=1000\n",
    "max_loss = 1e9\n",
    "\n",
    "# Track losses\n",
    "train_losses = []\n",
    "running_train_loss=[]\n",
    "val_losses = []\n",
    "steps = []\n",
    "val_steps=[]\n",
    "\n",
    "# Define optimizer\n",
    "params_to_optimize = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "optimizer = Adam8bit(params_to_optimize, lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_steps,\n",
    ")\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "global_step= 0\n",
    "\n",
    "while global_step< max_steps:\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        model.config.use_cache = False\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'].to('cuda'), attention_mask=batch['attention_mask'].to('cuda'), labels=batch['labels'].to('cuda'))\n",
    "        loss = outputs.loss\n",
    "        running_train_loss.append(loss.item())\n",
    "        # loss = loss / gradient_accumulation_steps  # Normalize loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # if (step + 1) % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        global_step += 1\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "        \n",
    "        print(f\"Step {global_step}/{max_steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            flush()\n",
    "            # Store train loss\n",
    "            train_losses.append(np.mean(running_train_loss[-20:]))\n",
    "            steps.append(global_step)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            count = 0\n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(test_dataloader, total=len(test_dataloader), leave=True)\n",
    "                for val_batch in pbar:\n",
    "                    val_outputs = model(\n",
    "                        input_ids=val_batch['input_ids'].to('cuda'),\n",
    "                        attention_mask=val_batch['attention_mask'].to('cuda'),\n",
    "                        labels=val_batch['labels'].to('cuda')\n",
    "                    )\n",
    "                    total_val_loss += val_outputs.loss.item()\n",
    "                    count += 1\n",
    "                    pbar.set_postfix({'val_loss': f'{(total_val_loss / count):.4f}'})\n",
    "            avg_val_loss = total_val_loss / count\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_steps.append(global_step)\n",
    "            print(f\"[Eval at step {global_step}] Val Loss: {avg_val_loss:.4f}\")\n",
    "            model.train()\n",
    "            # Plotting losses\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(running_train_loss[::len(val_steps)], label='Train Loss')\n",
    "            plt.plot(val_steps, val_losses, label='Val Loss')\n",
    "            plt.ylim([0,3])\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"train_val_loss_plot.png\")\n",
    "            plt.show()\n",
    "            \n",
    "        # if global_step % 20 == 0:\n",
    "        #     pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "        #     print('*'*20,step+1,'*'*20)\n",
    "        #     print(\"Predictions:\", pred)\n",
    "        #     print('*'*20,'end','*'*20)\n",
    "            \n",
    "        # if loss.item() < max_loss:\n",
    "        #     model.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')\n",
    "        #     max_loss = loss.item()\n",
    "                \n",
    "         \n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 300 ********************\n",
      "Predictions: <bos><bos><start_of_turn>user\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "କୋରିମୟକ ପ୍ର୍ର ଆ, ପରଚପୋକଷନ୍ ନତଚରା ପ୍ରେଲୋମ<end_of_turn>\n",
      "******************** end ********************\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "print('*'*20,step+1,'*'*20)\n",
    "print(\"Predictions:\", pred)\n",
    "print('*'*20,'end','*'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the LoRA and saving the Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_adapter.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the adapter into the base model\n",
    "model = PeftModel.from_pretrained(model, '/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ପୂର୍ବ-ଗୋଟିଏ ଖୈଳଧଙ୍ଚ, ଯେଉଁଠି ଅଂଘାଡ଼ି ସୃଷ୍ଟି ଥିଲା। ଆଞ୍ଚଳିକ ସୌନ୍ଦର୍ଯ୍ୟ ସଫର କରିଛି, ଓ ସ୍ଥାନୀୟ ଔପନିବେଶକମାନେ ପ୍ରଵେଶ କରୁଛନ୍ତି।\n",
      "\" \" - ଢ. ପ୍ରୋଫେସର ଐ. ରେନି (1928) | ଏହା ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ସମ୍ପର୍କିତ ଏକ ଜାରିବାର ପାଳନ କରୁଛି । ରାଜାରାଣୀ ଦୁ\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved checkpoint\n",
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(save_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Restore model, optimizer, scheduler, and step\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "global_step = checkpoint['global_step']\n",
    "\n",
    "# print(f\"Checkpoint loaded from {save_path} at step {global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = checkpoint['global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'global_step': global_step\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Checkpoint saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
