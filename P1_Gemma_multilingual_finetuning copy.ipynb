{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f58cfc0b834fdbaccc862fa9748baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3023.7419147491455\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoProcessor, BitsAndBytesConfig, get_scheduler,Gemma3ForConditionalGeneration\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"google/gemma-3-4b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation='eager',\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    "    \n",
    ")\n",
    "\n",
    "print(model.get_memory_footprint()/(1024*1024)) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Multi Lingual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"OdiaGenAI/hardcode_odia_qa_105\")\n",
    "dataset = load_dataset('OdiaGenAI/odia_domain_context_train_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଓଡ଼ିଶାର ଉଦୟଗିରି ଏବଂ ଖଣ୍ଡଗିରି ଗୁମ୍ଫାର ଇତିହାସ କ’ଣ?',\n",
       " 'input': '',\n",
       " 'output': 'ଉଦୟଗିରି ଏବଂ ଖଣ୍ଡଗିରି ଗୁମ୍ଫା ଭାରତର ଓଡ଼ିଶା ରାଜ୍ୟରେ ଅବସ୍ଥିତ ପ୍ରାଚୀନ ଗୁମ୍ଫା ପରିସରର ଏକ ସମୂହ। ଏହି ଗୁମ୍ଫା ଗୁଡିକ ଖ୍ରୀଷ୍ଟପୂର୍ବ ଦ୍ୱିତୀୟ ଶତାବ୍ଦୀ ଏବଂ ପ୍ରଥମ ଶତାବ୍ଦୀ ମଧ୍ୟରେ ଜୈନ ସନ୍ନ୍ୟାସୀମାନଙ୍କ ଦ୍ୱାରା ବାଲୁକା ପଥରରେ ଖୋଦିତ ହୋଇଥିବା ଜଣାଯାଏ।\\nଉଦୟଗିରି ଗୁମ୍ଫା ଗୁଡିକ ବୃହତ ଏବଂ ଅଧିକ ଜଟିଳ ହୋଇଥିବାବେଳେ ଖଣ୍ଡଗିରି ଗୁମ୍ଫା ଗୁଡିକ ଛୋଟ ଏବଂ ଡିଜାଇନ ଦୃଷ୍ଟିରୁ ସରଳ। ଏହି ପ୍ରାଚୀନ ଗୁମ୍ଫା ଗୁଡିକ ପ୍ରାଚୀନ ଭାରତୀୟ ସଭ୍ୟତାର ସ୍ଥାପତ୍ୟ କୌଶଳ ଏବଂ କଳାତ୍ମକ ଶ୍ରେଷ୍ଠତାର ପ୍ରମାଣ।\\nବିଗତ ବର୍ଷମାନଙ୍କରେ ଏହି ଗୁମ୍ଫା ସାରା ବିଶ୍ୱରୁ ପର୍ଯ୍ୟଟକଙ୍କ ଆକର୍ଷଣ କେନ୍ଦ୍ର ପାଲଟିଛି ଏବଂ ଭାରତୀୟ ପ୍ରତ୍ନତାତ୍ୱିକ ସର୍ବେକ୍ଷଣ ଦ୍ୱାରା ସଂରକ୍ଷିତ ହୋଇଛି। ଭାରତର ସମୃଦ୍ଧ ସାଂସ୍କୃତିକ ଐତିହ୍ୟ ପ୍ରତି ଆଗ୍ରହୀ ଯେକୌଣସି ବ୍ୟକ୍ତି ଏହି ଗୁମ୍ଫାକୁ ଦେଖିବା ଉଚିତ।'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(tokenizer(example['output'])['input_ids']) for example in dataset['train']]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Tokenized Output Lengths\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"what is capital of france?\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.float16)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "# outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user\\nYou are a helpful assistant\\n\\nwhat is capital of france?<end_of_turn>\\n<start_of_turn>model\\nThe capital of France is **Paris**. 😊 \\n\\nDo you want to know anything else about Paris, or perhaps France in general?<end_of_turn>']\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Object for Pytorch-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class LlamaDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        question=sample['instruction']\n",
    "        answer = sample['output']\n",
    "        prompt = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "        full_text = prompt+f'''{answer}<end_of_turn>'''\n",
    "\n",
    "        tokenized = tokenizer(full_text, truncation=True, add_special_tokens=False, padding=\"max_length\", max_length=200)\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        # Tokenize just the prompt to get the split point\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        answer_start = len(prompt_ids)\n",
    "\n",
    "        # Mask everything before answer_start\n",
    "        labels = [-100] * answer_start + input_ids[answer_start:]\n",
    "        # Mask out padding as well\n",
    "        labels = [\n",
    "            label if token != tokenizer.pad_token_id else -100\n",
    "            for label, token in zip(labels, input_ids)\n",
    "        ]\n",
    "    \n",
    "        return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LlamaDataset(dataset['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_tower.vision_model.embeddings.patch_embedding.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.embeddings.patch_embedding.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.embeddings.position_embedding.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "vision_tower.vision_model.post_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "vision_tower.vision_model.post_layernorm.bias  dtype: torch.bfloat16  requirs grad:  True\n",
      "multi_modal_projector.mm_input_projection_weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "multi_modal_projector.mm_soft_emb_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.0.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.1.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.2.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.3.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.4.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.5.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.6.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.7.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.8.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.9.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.10.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.11.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.12.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.13.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.14.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.15.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.16.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.16.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.17.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.17.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.18.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.18.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.19.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.19.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.20.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.20.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.21.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.21.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.22.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.22.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.23.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.23.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.24.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.24.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.25.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.25.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.26.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.26.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.27.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.27.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.28.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.28.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.29.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.29.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.30.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.30.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.31.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.31.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.32.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.32.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "language_model.model.layers.33.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.layers.33.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "language_model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of `prepare_model_for_kbit_training`\n",
    "\n",
    "When you load a model in 4-bit or 8-bit precision using bitsandbytes, some layers (like LayerNorm) still remain in full precision (float32), and certain operations (like weight updates) can be unstable or incompatible if done blindly on quantized weights.\n",
    "\n",
    "- Casts `LayerNorm` layers to `float32`\n",
    "- Sets `requires_grad=False` for all model parameters\n",
    "- Wraps the output layer (like `lm_head`) in `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language_model.model.layers.0.self_attn.q_proj',\n",
       " 'language_model.model.layers.0.self_attn.k_proj',\n",
       " 'language_model.model.layers.0.self_attn.v_proj',\n",
       " 'language_model.model.layers.0.self_attn.o_proj',\n",
       " 'language_model.model.layers.0.mlp.gate_proj',\n",
       " 'language_model.model.layers.0.mlp.up_proj',\n",
       " 'language_model.model.layers.0.mlp.down_proj',\n",
       " 'language_model.model.layers.1.self_attn.q_proj',\n",
       " 'language_model.model.layers.1.self_attn.k_proj',\n",
       " 'language_model.model.layers.1.self_attn.v_proj',\n",
       " 'language_model.model.layers.1.self_attn.o_proj',\n",
       " 'language_model.model.layers.1.mlp.gate_proj',\n",
       " 'language_model.model.layers.1.mlp.up_proj',\n",
       " 'language_model.model.layers.1.mlp.down_proj',\n",
       " 'language_model.model.layers.2.self_attn.q_proj',\n",
       " 'language_model.model.layers.2.self_attn.k_proj',\n",
       " 'language_model.model.layers.2.self_attn.v_proj',\n",
       " 'language_model.model.layers.2.self_attn.o_proj',\n",
       " 'language_model.model.layers.2.mlp.gate_proj',\n",
       " 'language_model.model.layers.2.mlp.up_proj',\n",
       " 'language_model.model.layers.2.mlp.down_proj',\n",
       " 'language_model.model.layers.3.self_attn.q_proj',\n",
       " 'language_model.model.layers.3.self_attn.k_proj',\n",
       " 'language_model.model.layers.3.self_attn.v_proj',\n",
       " 'language_model.model.layers.3.self_attn.o_proj',\n",
       " 'language_model.model.layers.3.mlp.gate_proj',\n",
       " 'language_model.model.layers.3.mlp.up_proj',\n",
       " 'language_model.model.layers.3.mlp.down_proj',\n",
       " 'language_model.model.layers.4.self_attn.q_proj',\n",
       " 'language_model.model.layers.4.self_attn.k_proj',\n",
       " 'language_model.model.layers.4.self_attn.v_proj',\n",
       " 'language_model.model.layers.4.self_attn.o_proj',\n",
       " 'language_model.model.layers.4.mlp.gate_proj',\n",
       " 'language_model.model.layers.4.mlp.up_proj',\n",
       " 'language_model.model.layers.4.mlp.down_proj',\n",
       " 'language_model.model.layers.5.self_attn.q_proj',\n",
       " 'language_model.model.layers.5.self_attn.k_proj',\n",
       " 'language_model.model.layers.5.self_attn.v_proj',\n",
       " 'language_model.model.layers.5.self_attn.o_proj',\n",
       " 'language_model.model.layers.5.mlp.gate_proj',\n",
       " 'language_model.model.layers.5.mlp.up_proj',\n",
       " 'language_model.model.layers.5.mlp.down_proj',\n",
       " 'language_model.model.layers.6.self_attn.q_proj',\n",
       " 'language_model.model.layers.6.self_attn.k_proj',\n",
       " 'language_model.model.layers.6.self_attn.v_proj',\n",
       " 'language_model.model.layers.6.self_attn.o_proj',\n",
       " 'language_model.model.layers.6.mlp.gate_proj',\n",
       " 'language_model.model.layers.6.mlp.up_proj',\n",
       " 'language_model.model.layers.6.mlp.down_proj',\n",
       " 'language_model.model.layers.7.self_attn.q_proj',\n",
       " 'language_model.model.layers.7.self_attn.k_proj',\n",
       " 'language_model.model.layers.7.self_attn.v_proj',\n",
       " 'language_model.model.layers.7.self_attn.o_proj',\n",
       " 'language_model.model.layers.7.mlp.gate_proj',\n",
       " 'language_model.model.layers.7.mlp.up_proj',\n",
       " 'language_model.model.layers.7.mlp.down_proj',\n",
       " 'language_model.model.layers.8.self_attn.q_proj',\n",
       " 'language_model.model.layers.8.self_attn.k_proj',\n",
       " 'language_model.model.layers.8.self_attn.v_proj',\n",
       " 'language_model.model.layers.8.self_attn.o_proj',\n",
       " 'language_model.model.layers.8.mlp.gate_proj',\n",
       " 'language_model.model.layers.8.mlp.up_proj',\n",
       " 'language_model.model.layers.8.mlp.down_proj',\n",
       " 'language_model.model.layers.9.self_attn.q_proj',\n",
       " 'language_model.model.layers.9.self_attn.k_proj',\n",
       " 'language_model.model.layers.9.self_attn.v_proj',\n",
       " 'language_model.model.layers.9.self_attn.o_proj',\n",
       " 'language_model.model.layers.9.mlp.gate_proj',\n",
       " 'language_model.model.layers.9.mlp.up_proj',\n",
       " 'language_model.model.layers.9.mlp.down_proj',\n",
       " 'language_model.model.layers.10.self_attn.q_proj',\n",
       " 'language_model.model.layers.10.self_attn.k_proj',\n",
       " 'language_model.model.layers.10.self_attn.v_proj',\n",
       " 'language_model.model.layers.10.self_attn.o_proj',\n",
       " 'language_model.model.layers.10.mlp.gate_proj',\n",
       " 'language_model.model.layers.10.mlp.up_proj',\n",
       " 'language_model.model.layers.10.mlp.down_proj',\n",
       " 'language_model.model.layers.11.self_attn.q_proj',\n",
       " 'language_model.model.layers.11.self_attn.k_proj',\n",
       " 'language_model.model.layers.11.self_attn.v_proj',\n",
       " 'language_model.model.layers.11.self_attn.o_proj',\n",
       " 'language_model.model.layers.11.mlp.gate_proj',\n",
       " 'language_model.model.layers.11.mlp.up_proj',\n",
       " 'language_model.model.layers.11.mlp.down_proj',\n",
       " 'language_model.model.layers.12.self_attn.q_proj',\n",
       " 'language_model.model.layers.12.self_attn.k_proj',\n",
       " 'language_model.model.layers.12.self_attn.v_proj',\n",
       " 'language_model.model.layers.12.self_attn.o_proj',\n",
       " 'language_model.model.layers.12.mlp.gate_proj',\n",
       " 'language_model.model.layers.12.mlp.up_proj',\n",
       " 'language_model.model.layers.12.mlp.down_proj',\n",
       " 'language_model.model.layers.13.self_attn.q_proj',\n",
       " 'language_model.model.layers.13.self_attn.k_proj',\n",
       " 'language_model.model.layers.13.self_attn.v_proj',\n",
       " 'language_model.model.layers.13.self_attn.o_proj',\n",
       " 'language_model.model.layers.13.mlp.gate_proj',\n",
       " 'language_model.model.layers.13.mlp.up_proj',\n",
       " 'language_model.model.layers.13.mlp.down_proj',\n",
       " 'language_model.model.layers.14.self_attn.q_proj',\n",
       " 'language_model.model.layers.14.self_attn.k_proj',\n",
       " 'language_model.model.layers.14.self_attn.v_proj',\n",
       " 'language_model.model.layers.14.self_attn.o_proj',\n",
       " 'language_model.model.layers.14.mlp.gate_proj',\n",
       " 'language_model.model.layers.14.mlp.up_proj',\n",
       " 'language_model.model.layers.14.mlp.down_proj',\n",
       " 'language_model.model.layers.15.self_attn.q_proj',\n",
       " 'language_model.model.layers.15.self_attn.k_proj',\n",
       " 'language_model.model.layers.15.self_attn.v_proj',\n",
       " 'language_model.model.layers.15.self_attn.o_proj',\n",
       " 'language_model.model.layers.15.mlp.gate_proj',\n",
       " 'language_model.model.layers.15.mlp.up_proj',\n",
       " 'language_model.model.layers.15.mlp.down_proj',\n",
       " 'language_model.model.layers.16.self_attn.q_proj',\n",
       " 'language_model.model.layers.16.self_attn.k_proj',\n",
       " 'language_model.model.layers.16.self_attn.v_proj',\n",
       " 'language_model.model.layers.16.self_attn.o_proj',\n",
       " 'language_model.model.layers.16.mlp.gate_proj',\n",
       " 'language_model.model.layers.16.mlp.up_proj',\n",
       " 'language_model.model.layers.16.mlp.down_proj',\n",
       " 'language_model.model.layers.17.self_attn.q_proj',\n",
       " 'language_model.model.layers.17.self_attn.k_proj',\n",
       " 'language_model.model.layers.17.self_attn.v_proj',\n",
       " 'language_model.model.layers.17.self_attn.o_proj',\n",
       " 'language_model.model.layers.17.mlp.gate_proj',\n",
       " 'language_model.model.layers.17.mlp.up_proj',\n",
       " 'language_model.model.layers.17.mlp.down_proj',\n",
       " 'language_model.model.layers.18.self_attn.q_proj',\n",
       " 'language_model.model.layers.18.self_attn.k_proj',\n",
       " 'language_model.model.layers.18.self_attn.v_proj',\n",
       " 'language_model.model.layers.18.self_attn.o_proj',\n",
       " 'language_model.model.layers.18.mlp.gate_proj',\n",
       " 'language_model.model.layers.18.mlp.up_proj',\n",
       " 'language_model.model.layers.18.mlp.down_proj',\n",
       " 'language_model.model.layers.19.self_attn.q_proj',\n",
       " 'language_model.model.layers.19.self_attn.k_proj',\n",
       " 'language_model.model.layers.19.self_attn.v_proj',\n",
       " 'language_model.model.layers.19.self_attn.o_proj',\n",
       " 'language_model.model.layers.19.mlp.gate_proj',\n",
       " 'language_model.model.layers.19.mlp.up_proj',\n",
       " 'language_model.model.layers.19.mlp.down_proj',\n",
       " 'language_model.model.layers.20.self_attn.q_proj',\n",
       " 'language_model.model.layers.20.self_attn.k_proj',\n",
       " 'language_model.model.layers.20.self_attn.v_proj',\n",
       " 'language_model.model.layers.20.self_attn.o_proj',\n",
       " 'language_model.model.layers.20.mlp.gate_proj',\n",
       " 'language_model.model.layers.20.mlp.up_proj',\n",
       " 'language_model.model.layers.20.mlp.down_proj',\n",
       " 'language_model.model.layers.21.self_attn.q_proj',\n",
       " 'language_model.model.layers.21.self_attn.k_proj',\n",
       " 'language_model.model.layers.21.self_attn.v_proj',\n",
       " 'language_model.model.layers.21.self_attn.o_proj',\n",
       " 'language_model.model.layers.21.mlp.gate_proj',\n",
       " 'language_model.model.layers.21.mlp.up_proj',\n",
       " 'language_model.model.layers.21.mlp.down_proj',\n",
       " 'language_model.model.layers.22.self_attn.q_proj',\n",
       " 'language_model.model.layers.22.self_attn.k_proj',\n",
       " 'language_model.model.layers.22.self_attn.v_proj',\n",
       " 'language_model.model.layers.22.self_attn.o_proj',\n",
       " 'language_model.model.layers.22.mlp.gate_proj',\n",
       " 'language_model.model.layers.22.mlp.up_proj',\n",
       " 'language_model.model.layers.22.mlp.down_proj',\n",
       " 'language_model.model.layers.23.self_attn.q_proj',\n",
       " 'language_model.model.layers.23.self_attn.k_proj',\n",
       " 'language_model.model.layers.23.self_attn.v_proj',\n",
       " 'language_model.model.layers.23.self_attn.o_proj',\n",
       " 'language_model.model.layers.23.mlp.gate_proj',\n",
       " 'language_model.model.layers.23.mlp.up_proj',\n",
       " 'language_model.model.layers.23.mlp.down_proj',\n",
       " 'language_model.model.layers.24.self_attn.q_proj',\n",
       " 'language_model.model.layers.24.self_attn.k_proj',\n",
       " 'language_model.model.layers.24.self_attn.v_proj',\n",
       " 'language_model.model.layers.24.self_attn.o_proj',\n",
       " 'language_model.model.layers.24.mlp.gate_proj',\n",
       " 'language_model.model.layers.24.mlp.up_proj',\n",
       " 'language_model.model.layers.24.mlp.down_proj',\n",
       " 'language_model.model.layers.25.self_attn.q_proj',\n",
       " 'language_model.model.layers.25.self_attn.k_proj',\n",
       " 'language_model.model.layers.25.self_attn.v_proj',\n",
       " 'language_model.model.layers.25.self_attn.o_proj',\n",
       " 'language_model.model.layers.25.mlp.gate_proj',\n",
       " 'language_model.model.layers.25.mlp.up_proj',\n",
       " 'language_model.model.layers.25.mlp.down_proj',\n",
       " 'language_model.model.layers.26.self_attn.q_proj',\n",
       " 'language_model.model.layers.26.self_attn.k_proj',\n",
       " 'language_model.model.layers.26.self_attn.v_proj',\n",
       " 'language_model.model.layers.26.self_attn.o_proj',\n",
       " 'language_model.model.layers.26.mlp.gate_proj',\n",
       " 'language_model.model.layers.26.mlp.up_proj',\n",
       " 'language_model.model.layers.26.mlp.down_proj',\n",
       " 'language_model.model.layers.27.self_attn.q_proj',\n",
       " 'language_model.model.layers.27.self_attn.k_proj',\n",
       " 'language_model.model.layers.27.self_attn.v_proj',\n",
       " 'language_model.model.layers.27.self_attn.o_proj',\n",
       " 'language_model.model.layers.27.mlp.gate_proj',\n",
       " 'language_model.model.layers.27.mlp.up_proj',\n",
       " 'language_model.model.layers.27.mlp.down_proj',\n",
       " 'language_model.model.layers.28.self_attn.q_proj',\n",
       " 'language_model.model.layers.28.self_attn.k_proj',\n",
       " 'language_model.model.layers.28.self_attn.v_proj',\n",
       " 'language_model.model.layers.28.self_attn.o_proj',\n",
       " 'language_model.model.layers.28.mlp.gate_proj',\n",
       " 'language_model.model.layers.28.mlp.up_proj',\n",
       " 'language_model.model.layers.28.mlp.down_proj',\n",
       " 'language_model.model.layers.29.self_attn.q_proj',\n",
       " 'language_model.model.layers.29.self_attn.k_proj',\n",
       " 'language_model.model.layers.29.self_attn.v_proj',\n",
       " 'language_model.model.layers.29.self_attn.o_proj',\n",
       " 'language_model.model.layers.29.mlp.gate_proj',\n",
       " 'language_model.model.layers.29.mlp.up_proj',\n",
       " 'language_model.model.layers.29.mlp.down_proj',\n",
       " 'language_model.model.layers.30.self_attn.q_proj',\n",
       " 'language_model.model.layers.30.self_attn.k_proj',\n",
       " 'language_model.model.layers.30.self_attn.v_proj',\n",
       " 'language_model.model.layers.30.self_attn.o_proj',\n",
       " 'language_model.model.layers.30.mlp.gate_proj',\n",
       " 'language_model.model.layers.30.mlp.up_proj',\n",
       " 'language_model.model.layers.30.mlp.down_proj',\n",
       " 'language_model.model.layers.31.self_attn.q_proj',\n",
       " 'language_model.model.layers.31.self_attn.k_proj',\n",
       " 'language_model.model.layers.31.self_attn.v_proj',\n",
       " 'language_model.model.layers.31.self_attn.o_proj',\n",
       " 'language_model.model.layers.31.mlp.gate_proj',\n",
       " 'language_model.model.layers.31.mlp.up_proj',\n",
       " 'language_model.model.layers.31.mlp.down_proj',\n",
       " 'language_model.model.layers.32.self_attn.q_proj',\n",
       " 'language_model.model.layers.32.self_attn.k_proj',\n",
       " 'language_model.model.layers.32.self_attn.v_proj',\n",
       " 'language_model.model.layers.32.self_attn.o_proj',\n",
       " 'language_model.model.layers.32.mlp.gate_proj',\n",
       " 'language_model.model.layers.32.mlp.up_proj',\n",
       " 'language_model.model.layers.32.mlp.down_proj',\n",
       " 'language_model.model.layers.33.self_attn.q_proj',\n",
       " 'language_model.model.layers.33.self_attn.k_proj',\n",
       " 'language_model.model.layers.33.self_attn.v_proj',\n",
       " 'language_model.model.layers.33.self_attn.o_proj',\n",
       " 'language_model.model.layers.33.mlp.gate_proj',\n",
       " 'language_model.model.layers.33.mlp.up_proj',\n",
       " 'language_model.model.layers.33.mlp.down_proj']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if \"vision_tower.vision_model\" not in name:  # Exclude vision tower\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # You might want to be more specific and only target attention and MLP layers\n",
    "            if any(layer_name in name for layer_name in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]):\n",
    "                target_modules.append(name) # Get the leaf node name\n",
    "target_modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,153,920 || all params: 4,431,233,392 || trainable%: 2.9598\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vision_tower.vision_model.embeddings.patch_embedding.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.embeddings.patch_embedding.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.embeddings.position_embedding.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.base_layer.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.post_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.vision_tower.vision_model.post_layernorm.bias  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.multi_modal_projector.mm_input_projection_weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.multi_modal_projector.mm_soft_emb_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.0.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.1.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.2.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.3.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.4.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.5.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.6.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.7.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.8.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.9.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.10.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.11.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.12.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.13.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.14.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.15.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.16.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.16.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.17.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.17.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.18.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.18.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.19.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.19.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.20.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.20.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.21.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.21.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.22.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.22.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.23.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.23.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.24.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.24.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.25.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.25.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.26.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.26.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.27.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.27.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.28.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.28.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.29.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.29.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.30.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.30.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.31.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.31.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.32.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.32.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.self_attn.q_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.self_attn.k_norm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.language_model.model.layers.33.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.pre_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.layers.33.post_feedforward_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.language_model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = '''ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର କିପରି ମିଳିମିଶି କାର୍ଯ୍ୟ କରିପାରିବେ?'''\n",
    "# answer = '''ଯେକୌଣସି ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ମିଳିତ ପ୍ରୟାସର ଆବଶ୍ୟକତା ରହିଛି। ଓଡ଼ିଶାର ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ବିସ୍ତୃତ ରଣନୀତି ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିବା ଲାଗି ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିପାରିବେ।\n",
    "# ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହିତ ମିଶି କାମ କରିବାର ଗୋଟିଏ ଉପାୟ ହେଲା ଘରୋଇ ନିବେଶ ପାଇଁ ଅନୁକୂଳ ବାତାବରଣ ସୃଷ୍ଟି କରିବା, ଏଥିରେ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ଟିକସ ଏବଂ ନିୟାମକ ପ୍ରତିବନ୍ଧକକୁ ହ୍ରାସ କରିବା, ଏହା ବ୍ୟତୀତ ସରକାର ଟିକସ ରିହାତି, ସବସିଡି ଏବଂ ପର୍ଯ୍ୟଟନ ବିକାଶ ପ୍ରକଳ୍ପ ପାଇଁ ଜମି ଆଦି ପ୍ରୋତ୍ସାହନ ମଧ୍ୟ ପ୍ରଦାନ କରିପାରିବେ।\n",
    "# ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ନୂତନ ପର୍ଯ୍ୟଟନ ଉତ୍ପାଦ ପ୍ରସ୍ତୁତ କରିପାରିବେ ଯାହା ଉଭୟ ଘରୋଇ ଏବଂ ଅନ୍ତର୍ଜାତୀୟ ପର୍ଯ୍ୟଟକଙ୍କ ଆବଶ୍ୟକତା ପୂରଣ କରିପାରିବ।\n",
    "# ସରକାର ମଧ୍ୟ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ପ୍ରଯୁକ୍ତିର ଉପଯୋଗ କରିପାରିବେ। ଉଦାହରଣ ସ୍ୱରୂପ, ସରକାର ପର୍ଯ୍ୟଟନ ସ୍ଥଳକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ଏବଂ ସମ୍ଭାବ୍ୟ ପର୍ଯ୍ୟଟକମାନଙ୍କ ସହିତ ଯୋଡ଼ିବା ଲାଗି ସୋସିଆଲ ମିଡିଆ ପ୍ଲାଟଫର୍ମର ଉପଯୋଗ କରିପାରିବେ। ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନ ଆକର୍ଷଣ ଏବଂ ଅନୁଭବ ପ୍ରଦର୍ଶିତ କରିବା ଲାଗି ଏକ ଅନଲାଇନ ପ୍ଲାଟଫର୍ମ ପ୍ରତିଷ୍ଠା କରିବା ଦ୍ୱାରା ଅଧିକ ପର୍ଯ୍ୟଟକଙ୍କୁ ଆକର୍ଷିତ କରିବାରେ ସହାୟତା ମିଳିପାରିବ।\n",
    "# ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିବାକୁ ପଡିବ ଯେପରିକି ପର୍ଯ୍ୟଟନ ଗତିବିଧି ଦ୍ୱାରା ପର୍ଯ୍ୟାବରଣର କ୍ଷୟ କିମ୍ବା ସ୍ଥାନୀୟ ସମ୍ପ୍ରଦାୟର କ୍ଷତି ନ ହେଉ।\n",
    "# ଶେଷରେ, ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ଅନୁକୂଳ ପରିବେଶ ସୃଷ୍ଟି କରିବା ଆବଶ୍ୟକ ଏବଂ ଏହା ସୁନିଶ୍ଚିତ କରିବା ଉଚିତ ଯେ ବିକାଶ ସ୍ଥାୟୀ ହେବ। ” ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରି ସେମାନେ ପର୍ଯ୍ୟଟନ ରଣନୀତିକୁ ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିପାରିବେ ଯାହା କେବଳ ପର୍ଯ୍ୟଟନ ଉଦ୍ୟୋଗ ନୁହେଁ ବରଂ ସ୍ଥାନୀୟ ଗୋଷ୍ଠୀ ଏବଂ ପରିବେଶକୁ ମଧ୍ୟ ଲାଭାନ୍ୱିତ କରିବ।'''\n",
    "\n",
    "# tokenized_text = tokenizer(answer).input_ids\n",
    "# print(len(tokenized_text))\n",
    "# for idx in range(len(tokenized_text)):\n",
    "#     clear_output(wait=True)\n",
    "#     print(tokenizer.decode(tokenized_text[0:idx]))\n",
    "#     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the LLAMA model on a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"user\\nପ୍ରାରମ୍ଭିକ ଏଏଆଇ ଇମ୍ୟୁନୋଲୋଜି ପାଠ୍ୟକ୍ରମ କ 'ଣ କରୁଛି?\\nmodel\\nଛାତ୍ରଛାତ୍ରୀଙ୍କୁ ରୋଗ ପ୍ରତିରୋଧକ ଶକ୍ତି ବିଷୟରେ ଅବଗତ କରାଇଥାଏ\",\n",
       " \"user\\nକୋରସାକୋଫ୍ ସିଣ୍ଡ୍ରୋମ କ 'ଣ?\\nmodel\\nଏକ ଜୈବିକ ମସ୍ତିଷ୍କ ରୋଗ ଯାହା ପ୍ରିଫ୍ରଣ୍ଟଲ କର୍ଟେକ୍ସ ମଧ୍ୟରେ ସ୍ନାୟୁକୋଷର ବ୍ୟାପକ କ୍ଷତି କିମ୍ବା ସଙ୍କୋଚନ ଦ୍ୱାରା ସ୍ମୃତିଶକ୍ତିକୁ ପ୍ରତିକୂଳ ଭାବେ ପ୍ରଭାବିତ କରିଥାଏ।\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "tokenizer.batch_decode(batch['input_ids'],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(model,idx=5,disable_lora=False):\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    sample=dataset['train'][idx]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<bos><start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to(device).to(torch.float16)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if disable_lora:\n",
    "        with model.disable_adapter():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                max_new_tokens=256,\n",
    "                repetition_penalty=1.3,\n",
    "                temperature=0.7,         # Optional: smooth randomness\n",
    "                top_k=50,                # Optional: top-k sampling\n",
    "                top_p=0.9                # Optional: nucleus sampling\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=256,\n",
    "        repetition_penalty=1.3,\n",
    "        temperature=0.7,         # Optional: smooth randomness\n",
    "        top_k=50,                # Optional: top-k sampling\n",
    "        top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?',\n",
       " 'input': '',\n",
       " 'output': 'ଏକାଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମାଣ କରାଯାଇଥିବା ରାଜାରାଣୀ ମନ୍ଦିର ଭୁବନେଶ୍ୱର ସହରର ଏକ ଲୋକପ୍ରିୟ ପର୍ଯ୍ୟଟନସ୍ଥଳୀ।\\nଏକାଦଶ ଶତାବ୍ଦୀରେ ଏହି ଅଂଚଳରେ ରାଜାରାଣୀ ରାଜବଂଶ ଶାସନ କରିଥିଲେ। ବିଶ୍ୱାସ କରାଯାଏ ଯେ ଏହି ମନ୍ଦିର ପ୍ରଥମେ ଭଗବାନ ଶିବଙ୍କୁ ସମର୍ପିତ ଥିଲା, କିନ୍ତୁ ପରେ ଏହା ଭଗବାନ ବିଷ୍ଣୁଙ୍କ ମନ୍ଦିର ପାଲଟିଥିଲା।\\nରାଜାରାଣୀ ମନ୍ଦିରର ସ୍ଥାପତ୍ୟ ଅଦ୍ୱିତୀୟ, କାରଣ ଏଠାରେ କୌଣସି ପବିତ୍ର ସ୍ଥାନ କିମ୍ବା କେନ୍ଦ୍ରୀୟ ମନ୍ଦିର ନାହିଁ, ଯାହାକି ହିନ୍ଦୁ ମନ୍ଦିରର ଏକ ବିଶେଷ ବୈଶିଷ୍ଟ୍ୟ, ଏହା ପରିବର୍ତ୍ତେ ମନ୍ଦିରର ଚାରିପଟେ ଛୋଟ ଛୋଟ ମନ୍ଦିର ରହିଛି ଯେଉଁଥିରେ ବିଭିନ୍ନ ଦେବଦେବୀଙ୍କ ମୂର୍ତ୍ତି ରହିଛି।\\nଏହି ମନ୍ଦିରର ଅନ୍ୟ ଏକ ବିଶେଷତ୍ୱ ହେଉଛି ଏହାର ସୁସଜ୍ଜିତ ଭାସ୍କର୍ଯ୍ୟ ଏବଂ ଖୋଦାଇ କାର୍ଯ୍ୟ। ଏହି ମନ୍ଦିରରେ ଦେବ-ଦେବୀ ଏବଂ ପୌରାଣିକ ପ୍ରାଣୀମାନଙ୍କ ଜଟିଳ ଚିତ୍ରଣ ରହିଛି, ଯାହା ଏହାକୁ କଳା ଏବଂ ସ୍ଥାପତ୍ୟର ଉତ୍ସାହୀମାନଙ୍କ ପାଇଁ ଏକ ଲୋକପ୍ରିୟ ଗନ୍ତବ୍ୟ ସ୍ଥଳୀରେ ପରିଣତ କରିଛି।\\nବିଗତ ବର୍ଷମାନଙ୍କରେ, ରାଜାରାଣୀ ମନ୍ଦିର ଭୁବନେଶ୍ୱରର ଏକ ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ସାଂସ୍କୃତିକ ସ୍ଥଳୀରେ ପରିଣତ ହୋଇଛି ଏବଂ ଏହି କ୍ଷେତ୍ରର ସମୃଦ୍ଧ ଇତିହାସ ଏବଂ ସାଂସ୍କୃତିକ ଐତିହ୍ୟର ଏକ ଉଦାହରଣ।'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ମୋଟ ଦାନଗ ବିଲ୍ୟ ସଂହିତ ଭୁବାଞ୍ଚଳ ଓଡ଼ଉ ପରିପୂର୍ଣ୍ଣ ପ୍ରକୃଷ୍ଟିଆଙ୍ଗ ବହୁଦାନଙ୍କ ଜନା ଅଧିକାର ଆୟତ୍ଵରେ ଗ୍ରନ୍ଥ ଶԱନ୍ତି ନିକଟ ଏକ ଯୋଜନାର ରକ୍ଷା ହୋଇଥିଲା । ଅନ୍ତଃସ୍ ଚାର୍କାର୍ଯ କଲେ ସମ୍ଘୃତା ଖନି, ଅପ୍ରାହିଳ ଶତ୍ରୁ, ସୌଳ ସମ୍ପର୍କ, ଏବଂ କ୍ରମ-କ୍ରମ ସୌଳ ସମ୍ପର୍କ ସଂଶ contested।\n",
      "\n",
      "ଅର୍ଥात् ଗ୍ରନ୍ଥ ଶԱନ୍ତି ନିକଟ ସଂପୂର୍ଣ୍ଣ ବି କରିଥିଲା। ଯେତନେଽଟି ମାଝାଗ୍ରୀ ଗ୍ରନ୍ଥ ଶԱନ୍ତିକୁ କାଇଁ ପାଠପଢ଼େ ସponsored କରିଥିଲା:\n",
      "*   ଜନହିଙ୍ଗ (1025 CE)\n",
      "*   ଏକା ଅଦବୀ (1086 CE)\n",
      "*   ଦାସା\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Loss: 0.4840\n",
      "Epoch 3/500, Loss: 0.5525\n",
      "Epoch 4/500, Loss: 0.3825\n",
      "Epoch 5/500, Loss: 0.4653\n",
      "Epoch 6/500, Loss: 0.5354\n",
      "Epoch 7/500, Loss: 0.2869\n",
      "Epoch 8/500, Loss: 0.5344\n",
      "Epoch 9/500, Loss: 0.4573\n",
      "Epoch 10/500, Loss: 0.4764\n",
      "Epoch 11/500, Loss: 0.5638\n",
      "Epoch 12/500, Loss: 0.4279\n",
      "Epoch 13/500, Loss: 0.4473\n",
      "Epoch 14/500, Loss: 0.4151\n",
      "Epoch 15/500, Loss: 0.4112\n",
      "Epoch 16/500, Loss: 0.4534\n",
      "Epoch 17/500, Loss: 0.4320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps  \u001b[38;5;66;03m# Normalize loss\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "gradient_accumulation_steps = 4\n",
    "max_steps=500\n",
    "max_loss = 1e9\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam8bit(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_steps,\n",
    ")\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "global_step= 0\n",
    "\n",
    "while global_step< max_steps:\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        model.config.use_cache = False\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'].to('cuda'), attention_mask=batch['attention_mask'].to('cuda'), labels=batch['labels'].to('cuda'))\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps  # Normalize loss\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        global_step += 1\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            pred = generate_eval(model=model,idx=40,disable_lora=False)\n",
    "            print('*'*20,step+1,'*'*20)\n",
    "            print(\"Predictions:\", pred)\n",
    "            print('*'*20,'end','*'*20)\n",
    "            \n",
    "        if loss.item() < max_loss:\n",
    "            model.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')\n",
    "            max_loss = loss.item()\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {global_step + 1}/{max_steps}, Loss: {loss.item():.4f}\")\n",
    "         \n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the LoRA and saving the Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_adapter.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the adapter into the base model\n",
    "model = PeftModel.from_pretrained(model, '/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଭୁବନେଶ୍ୱରର ରାଜାରାଣୀ ମନ୍ଦିରର ଇତିହାସ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ପୂର୍ବ-ଗୋଟିଏ ଖୈଳଧଙ୍ଚ, ଯେଉଁଠି ଅଂଘାଡ଼ି ସୃଷ୍ଟି ଥିଲା। ଆଞ୍ଚଳିକ ସୌନ୍ଦର୍ଯ୍ୟ ସଫର କରିଛି, ଓ ସ୍ଥାନୀୟ ଔପନିବେଶକମାନେ ପ୍ରଵେଶ କରୁଛନ୍ତି।\n",
      "\" \" - ଢ. ପ୍ରୋଫେସର ଐ. ରେନି (1928) | ଏହା ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ସମ୍ପର୍କିତ ଏକ ଜାରିବାର ପାଳନ କରୁଛି । ରାଜାରାଣୀ ଦୁ\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved checkpoint\n",
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(save_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Restore model, optimizer, scheduler, and step\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "global_step = checkpoint['global_step']\n",
    "\n",
    "# print(f\"Checkpoint loaded from {save_path} at step {global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = checkpoint['global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'global_step': global_step\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Checkpoint saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
