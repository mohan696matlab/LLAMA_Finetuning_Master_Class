{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 11:41:47.904263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745581307.913777 1627182 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745581307.916663 1627182 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-25 11:41:47.927021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "comet_ml is installed but the Comet API Key is not configured. Please set the `COMET_API_KEY` environment variable to enable Comet logging. Check out the documentation for other ways of configuring it: https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#set-the-api-key\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50431786585464cb39a317ab9b23400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2095.841064453125\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, get_scheduler\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc,os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from evaluation_utlis import rouge_bleu_score,get_prediction_per_sentence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "TOKENIZER_PATH = DEFAULT_MODEL#\"llama_odia_tokenizer\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "print(model.get_memory_footprint()/(1024*1024)) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"OdiaGenAI/hardcode_odia_qa_105\")\n",
    "dataset = load_dataset('OdiaGenAI/odia_domain_context_train_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mohan.dash/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['ଗੁਜ\\u200d্\\u200cર,', 'ଅ\\u200bস\\u200cफ़\\u200cग़\\u200d\\u200c\\u200c\\u200c\\u200dक़\\u200c', '\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c'],\nInput references: [['2019', 'ମସିହାରେ', 'ଓଡ଼ିଶାରେ', '1.5', 'କୋଟିରୁ', 'ଅଧିକ', 'ପର୍ଯ୍ୟଟକଙ୍କୁ', 'ସ୍ୱାଗତ', 'କରାଯାଇଛି', 'ଏବଂ', 'ଆଗାମୀ', 'ବର୍ଷମାନଙ୍କରେ', 'ଏହି', 'ସଂଖ୍ୟା', 'ବୃଦ୍ଧି', 'ପାଇବ', 'ବୋଲି', 'ଆଶା', 'କରାଯାଉଛି।', 'ଓଡ଼ିଶାରେ', 'ଅନେକ', 'ଲୋକପ୍ରିୟ', 'ପର୍ଯ୍ୟଟନ', 'ସ୍ଥଳୀ', 'ରହିଛି', 'ଯାହା', 'ପ୍ରତିବର୍ଷ', 'ପର୍ଯ୍ୟଟକଙ୍କୁ', 'ଆକର୍ଷିତ', 'କରିଥାଏ,', 'ଯେପରିକି', 'ପୁରୀ,', 'କୋଣାର୍କ', 'ଏବଂ', 'ଭୁବନେଶ୍ୱର।', 'କେବଳ', 'ସେତିକି', 'ନୁହେଁ,', 'ଇକୋ-ଟୁରିଜିମ,', 'ହେରିଟେଜ', 'ଟୁରିଜମ,', 'ଆଡଭେଞ୍ଚର', 'ଟୁରିଜମ', 'ଭଳି', 'ପର୍ଯ୍ୟଟନକୁ', 'ମଧ୍ୟ', 'ରାଜ୍ୟ', 'ସରକାର', 'ପ୍ରୋତ୍ସାହନ', 'ଦେଉଛନ୍ତି।', 'ସାମଗ୍ରୀକ', 'ଭାବେ', 'ଓଡ଼ିଶାରେ', 'ପର୍ଯ୍ୟଟନର', 'ବର୍ତ୍ତମାନର', 'ସ୍ଥିତି', 'ଆଶାପ୍ରଦ', 'ଏବଂ', 'ସରକାରୀ', 'ଓ', 'ଘରୋଇ', 'କ୍ଷେତ୍ରର', 'ନିରନ୍ତର', 'ପ୍ରୟାସ', 'ଫଳରେ', 'ଓଡ଼ିଶା', 'ଦେଶର', 'ପ୍ରମୁଖ', 'ପର୍ଯ୍ୟଟନ', 'ସ୍ଥଳୀମାନଙ୍କ', 'ମଧ୍ୟରୁ', 'ଅନ୍ୟତମ', 'ହେବାକୁ', 'ଯାଉଛି।']]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrouge_bleu_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m00\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLAMA_Finetuning_Master_Class/evaluation_utlis.py:58\u001b[0m, in \u001b[0;36mrouge_bleu_score\u001b[0;34m(model, tokenizer, dataset, current_step, max_new_tokens, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[1;32m     52\u001b[0m result_rouge \u001b[38;5;241m=\u001b[39m rouge\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m     53\u001b[0m     predictions\u001b[38;5;241m=\u001b[39mpred_texts, \n\u001b[1;32m     54\u001b[0m     references\u001b[38;5;241m=\u001b[39mtrue_texts, \n\u001b[1;32m     55\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m result_bleu \u001b[38;5;241m=\u001b[39m \u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred_texts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrue_texts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate average ROUGE score\u001b[39;00m\n\u001b[1;32m     64\u001b[0m avg_rouge_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mlist\u001b[39m(result_rouge\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/evaluate/module.py:514\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_feature_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/evaluate/module.py:596\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_feature_from_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/evaluate/module.py:616\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    609\u001b[0m feature_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)])\n\u001b[1;32m    610\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m )\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: ['ଗੁਜ\\u200d্\\u200cર,', 'ଅ\\u200bস\\u200cफ़\\u200cग़\\u200d\\u200c\\u200c\\u200c\\u200dक़\\u200c', '\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c\\u200c'],\nInput references: [['2019', 'ମସିହାରେ', 'ଓଡ଼ିଶାରେ', '1.5', 'କୋଟିରୁ', 'ଅଧିକ', 'ପର୍ଯ୍ୟଟକଙ୍କୁ', 'ସ୍ୱାଗତ', 'କରାଯାଇଛି', 'ଏବଂ', 'ଆଗାମୀ', 'ବର୍ଷମାନଙ୍କରେ', 'ଏହି', 'ସଂଖ୍ୟା', 'ବୃଦ୍ଧି', 'ପାଇବ', 'ବୋଲି', 'ଆଶା', 'କରାଯାଉଛି।', 'ଓଡ଼ିଶାରେ', 'ଅନେକ', 'ଲୋକପ୍ରିୟ', 'ପର୍ଯ୍ୟଟନ', 'ସ୍ଥଳୀ', 'ରହିଛି', 'ଯାହା', 'ପ୍ରତିବର୍ଷ', 'ପର୍ଯ୍ୟଟକଙ୍କୁ', 'ଆକର୍ଷିତ', 'କରିଥାଏ,', 'ଯେପରିକି', 'ପୁରୀ,', 'କୋଣାର୍କ', 'ଏବଂ', 'ଭୁବନେଶ୍ୱର।', 'କେବଳ', 'ସେତିକି', 'ନୁହେଁ,', 'ଇକୋ-ଟୁରିଜିମ,', 'ହେରିଟେଜ', 'ଟୁରିଜମ,', 'ଆଡଭେଞ୍ଚର', 'ଟୁରିଜମ', 'ଭଳି', 'ପର୍ଯ୍ୟଟନକୁ', 'ମଧ୍ୟ', 'ରାଜ୍ୟ', 'ସରକାର', 'ପ୍ରୋତ୍ସାହନ', 'ଦେଉଛନ୍ତି।', 'ସାମଗ୍ରୀକ', 'ଭାବେ', 'ଓଡ଼ିଶାରେ', 'ପର୍ଯ୍ୟଟନର', 'ବର୍ତ୍ତମାନର', 'ସ୍ଥିତି', 'ଆଶାପ୍ରଦ', 'ଏବଂ', 'ସରକାରୀ', 'ଓ', 'ଘରୋଇ', 'କ୍ଷେତ୍ରର', 'ନିରନ୍ତର', 'ପ୍ରୟାସ', 'ଫଳରେ', 'ଓଡ଼ିଶା', 'ଦେଶର', 'ପ୍ରମୁଖ', 'ପର୍ଯ୍ୟଟନ', 'ସ୍ଥଳୀମାନଙ୍କ', 'ମଧ୍ୟରୁ', 'ଅନ୍ୟତମ', 'ହେବାକୁ', 'ଯାଉଛି।']]"
     ]
    }
   ],
   "source": [
    "res = rouge_bleu_score(model,tokenizer,dataset['train'],current_step=00,max_new_tokens=100,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from evaluation_utlis import rouge_bleu_score,get_prediction_per_sentence\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Store predictions and ground truths\n",
    "pred_texts, true_texts = [], []\n",
    "output_lines = []\n",
    "\n",
    "# Collect examples\n",
    "for idx in [11, 111, 222, 333, 444]:\n",
    "    sample = dataset['train'][idx]\n",
    "    question, pred_text, true_text = get_prediction_per_sentence(model, tokenizer, sample, max_new_tokens=100, device=device)\n",
    "    pred_texts.append(pred_text)\n",
    "    true_texts.append(true_text)\n",
    "\n",
    "    # Format the result for the output file\n",
    "    output_lines.append(\"=========================================\")\n",
    "    output_lines.append(f\"Question:\\n{question}\\n\")\n",
    "    output_lines.append(f\"Ground Truth:\\n{true_text}\\n\")\n",
    "    output_lines.append(f\"Prediction:\\n{pred_text}\\n\")\n",
    "\n",
    "# Compute metrics\n",
    "result_rouge = rouge.compute(\n",
    "    predictions=pred_texts, \n",
    "    references=true_texts, \n",
    "    tokenizer=lambda x: x.split()\n",
    ")\n",
    "\n",
    "result_bleu = bleu.compute(\n",
    "    predictions=pred_texts, \n",
    "    references=true_texts, \n",
    ")\n",
    "\n",
    "# Calculate average ROUGE score\n",
    "avg_rouge_score = np.mean(list(result_rouge.values()))\n",
    "bleu_score = result_bleu['bleu']\n",
    "\n",
    "# Add metrics to output\n",
    "output_lines.append(\"=========================================\")\n",
    "output_lines.append(f\"ROUGE Score (avg): {avg_rouge_score:.4f}\")\n",
    "output_lines.append(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "# Save to a text file\n",
    "with open(f\"evaluation_results_{0}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "result_rouge = rouge.compute(\n",
    "    predictions=pred_texts, \n",
    "    references=true_texts, \n",
    "    tokenizer=lambda x: x.split()\n",
    ")\n",
    "\n",
    "result_bleu = bleu.compute(\n",
    "    predictions=pred_texts, \n",
    "    references=true_texts, \n",
    ")\n",
    "\n",
    "# Calculate average ROUGE score\n",
    "avg_rouge_score = np.mean(list(result_rouge.values()))\n",
    "bleu_score = result_bleu['bleu']\n",
    "\n",
    "# Add metrics to output\n",
    "output_lines.append(\"=========================================\")\n",
    "output_lines.append(f\"ROUGE Score (avg): {avg_rouge_score:.4f}\")\n",
    "output_lines.append(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "# Save to a text file\n",
    "with open(f\"evaluation_results_{0}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.12698412698412698, 0.017241379310344827, 0.0, 0.0],\n",
       " 'brevity_penalty': 7.665540221071256e-05,\n",
       " 'length_ratio': 0.09545454545454546,\n",
       " 'translation_length': 63,\n",
       " 'reference_length': 660}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_bleu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
