{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the LLAMA 3.2 with Pure Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA is a gated model and you will need to signin in using huggingface token to download the checkpoint (Only for the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357.1290283203125\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/(1024*1024)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to ask a question to LLAMA 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Italy?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=20, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is behind the Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
       "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
       "             25,    220,   1721,   3297,    220,   2366,     20,    271, 128009,\n",
       "         128006,    882, 128007,    271,   3923,    374,    279,   6864,    315,\n",
       "          15704,     30, 128009, 128006,  78191, 128007,    271]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(tokenized_text, return_tensors=\"pt\").to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(tokenized_text, return_tensors=\"pt\",padding='max_length',max_length=300).to(device)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 43, 128256]), torch.Size([1, 43]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 16309,  16309, 128006,    198,  78191,    567,   1303,    311,  22241,\n",
      "             25,    220,    220,     16,     15,    198,   9673,    596,     25,\n",
      "            510,   2360,   5936,    220,   2366,     18,    198,   2028, 128006,\n",
      "          78191,   3638,    271,    644,    374,    279,   1888,    315,   1561,\n",
      "           5380,  22463, 128006,  78191, 128007,    271,    791]],\n",
      "       device='cuda:0')\n",
      "tensor(791, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "print(predicted_ids)\n",
    "\n",
    "next_word_id = predicted_ids[0, -1]\n",
    "print(next_word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next word:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The predicted next word:')\n",
    "tokenizer.decode(next_word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_ids = torch.cat([inputs.input_ids, next_word_id.view((-1,1))], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next word:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(new_input_ids)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "next_word_id = predicted_ids[0, -1]\n",
    "\n",
    "new_input_ids = torch.cat([new_input_ids, next_word_id.view((-1,1))], dim=-1)\n",
    "\n",
    "print('The predicted next word:')\n",
    "print(tokenizer.batch_decode(new_input_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens when we ask something out of it's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I don't have information on the BG-XAI system, as my knowledge cutoff is 01 March\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=20, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train the model to learn this new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n",
      "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   1721,   3297,    220,   2366,     20,    271, 128009,\n",
      "         128006,    882, 128007,    271,   4438,   1587,  44111,  31650,  15836,\n",
      "           1520,    304,  14867,  23842,     30, 128009, 128006,  78191, 128007,\n",
      "            271,  38862,  31650,  15836,   5825,  41941,    369,  14867,  20492,\n",
      "           1701,    459,  18274,   9134,   6108,   1749,     11,  10695,   1304,\n",
      "          15592,   6108,  50518,    810,  49839,     13, 128009]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=False)# add_generation_prompt=False as we have already added it\n",
    "print(tokenized_text)\n",
    "\n",
    "inputs = tokenizer(tokenized_text, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift logits and labels for causal LM\n",
    "shift_logits = logits[:-1, :].contiguous()\n",
    "shift_labels = inputs[\"input_ids\"][:,1:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69, 128256]), torch.Size([1, 69]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits.shape,shift_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:08,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:07,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4921875\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:01<00:06,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.462890625\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:01<00:05,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1689453125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:01<00:05,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03857421875\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:02<00:04,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.031982421875\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:02<00:04,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0260009765625\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:02<00:04,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0238037109375\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:03<00:03,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0228271484375\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:03<00:03,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0220947265625\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:03<00:03,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0213623046875\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:04<00:02,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0208740234375\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:04<00:02,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02099609375\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:05<00:02,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0208740234375\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:05<00:01,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.020751953125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:05<00:01,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0205078125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:06<00:01,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0205078125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:06<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0205078125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:06<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0203857421875\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0206298828125\n",
      "GPU Memory Usage>>>> Allocated: 7106.77 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.train()\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = inputs[\"input_ids\"][:,1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print_gpu_utilization()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement-1\n",
    "### Calculate loss only on the 'RESPONSE' not on the 'QUERY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}]\n",
    "\n",
    "tokenized_query = tokenizer.apply_chat_template(query,tokenize=True,add_generation_prompt=True)\n",
    "\n",
    "full_message = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_full_message = tokenizer.apply_chat_template(full_message,tokenize=True,add_generation_prompt=False)\n",
    "\n",
    "labels = tokenized_full_message\n",
    "labels = [-100] * len(tokenized_query) + tokenized_full_message[len(tokenized_query):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(tokenized_full_message).unsqueeze(0).to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([l for l in labels if l!=-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 2390.51 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00021076202392578125\n",
      "GPU Memory Usage>>>> Allocated: 7106.52 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:01,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.25\n",
      "GPU Memory Usage>>>> Allocated: 7106.52 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.380859375\n",
      "GPU Memory Usage>>>> Allocated: 7106.52 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:01<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0038909912109375\n",
      "GPU Memory Usage>>>> Allocated: 7106.52 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.007568359375\n",
      "GPU Memory Usage>>>> Allocated: 7106.52 MB |||||  Reserved:  12410.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.train()\n",
    "print_gpu_utilization()\n",
    "for _ in tqdm(range(5)):\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LoRA\n",
    "![LoRA Concept](./images/lora.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters to optimize: 1000000\n",
      "LoRA parameters to optimize: 4000\n",
      "0.40% of total parameters\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(1000, 1000, dtype=torch.float32).cuda()\n",
    "print(f'Total parameters to optimize: {torch.numel(W)}')\n",
    "\n",
    "rank = 2\n",
    "A = torch.randn(1000, rank, dtype=torch.float32).cuda()\n",
    "B = torch.randn(rank, 1000, dtype=torch.float32).cuda()\n",
    "print(f'LoRA parameters to optimize: {torch.numel(A)+torch.numel(B)}')\n",
    "print(f'{(torch.numel(A)+torch.numel(B))/torch.numel(W)*100:.2f}% of total parameters')\n",
    "\n",
    "# Compute W = A @ B\n",
    "W_lora = A @ B\n",
    "print(torch.numel(W_lora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch, gc\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 0.00 MB |||||  Reserved:  0.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 2357.13 MB |||||  Reserved:  2862.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.norm.weight  dtype: torch.bfloat16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     inference_mode=False,\n",
    "#     use_rslora=True,\n",
    "#     init_lora_weights=\"gaussian\",\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "# Now manually move LoRA params to bf16\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "        if param.requires_grad:\n",
    "            param.grad = None  # Reset grads just in case\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}]\n",
    "\n",
    "tokenized_query = tokenizer.apply_chat_template(query,tokenize=True,add_generation_prompt=True)\n",
    "\n",
    "full_message = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_full_message = tokenizer.apply_chat_template(full_message,tokenize=True,add_generation_prompt=False)\n",
    "\n",
    "labels = tokenized_full_message\n",
    "labels = [-100] * len(tokenized_query) + tokenized_full_message[len(tokenized_query):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(tokenized_full_message).unsqueeze(0).to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([l for l in labels if l!=-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 2394.40 MB |||||  Reserved:  2904.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.78125\n",
      "GPU Memory Usage>>>> Allocated: 2462.41 MB |||||  Reserved:  2974.00 MB:\n",
      "Loss: 2.625\n",
      "GPU Memory Usage>>>> Allocated: 2462.41 MB |||||  Reserved:  3022.00 MB:\n",
      "Loss: 1.1484375\n",
      "GPU Memory Usage>>>> Allocated: 2462.41 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.361328125\n",
      "GPU Memory Usage>>>> Allocated: 2462.41 MB |||||  Reserved:  3022.00 MB:\n",
      "Loss: 0.1181640625\n",
      "GPU Memory Usage>>>> Allocated: 2462.41 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "print_gpu_utilization()\n",
    "for _ in tqdm(range(5)):\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    # flush()\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
      "GPU Memory Usage>>>> Allocated: 2462.41 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n",
    "import gc\n",
    "# flush()\n",
    "\n",
    "\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 3 (Q-LoRA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 382.00 MB |||||  Reserved:  382.00 MB:\n",
      "Original dtype: torch.float32\n",
      "GPU Memory Usage>>>> Allocated: 53.96 MB |||||  Reserved:  70.00 MB:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Create a simple 10000x10000 float32 tensor\n",
    "tensor_fp32 = torch.randn(10000, 10000, dtype=torch.float32).cuda()\n",
    "print_gpu_utilization()\n",
    "\n",
    "print(f\"Original dtype: {tensor_fp32.dtype}\")\n",
    "\n",
    "# Quantize to 8-bit using bitsandbytes\n",
    "# bnb.nn.Int8Params is designed to store tensors in int8\n",
    "quantized_tensor = bnb.functional.quantize_4bit(tensor_fp32)\n",
    "\n",
    "del tensor_fp32\n",
    "flush()\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([[ 0.0530, -0.6666, -0.2254,  ..., -0.0167, -1.3745, -1.0148],\n",
      "        [-1.9705, -1.6118, -0.8275,  ...,  0.2298,  1.0841,  0.1506],\n",
      "        [ 0.4698, -0.7303, -0.3588,  ..., -1.2706,  0.1451, -2.4177],\n",
      "        ...,\n",
      "        [ 1.1321,  0.2271,  0.9253,  ...,  0.7541, -0.9884, -0.8722],\n",
      "        [ 0.4753, -1.6584,  0.8973,  ...,  0.2806, -0.2771,  0.7411],\n",
      "        [-0.5635, -1.9339, -0.2042,  ...,  1.6012,  0.4941,  0.9101]],\n",
      "       device='cuda:0') \n",
      "\n",
      "Quantized: tensor([[ 31],\n",
      "        [238],\n",
      "        [127],\n",
      "        ...,\n",
      "        [152],\n",
      "        [181],\n",
      "        [103]], device='cuda:0', dtype=torch.uint8) \n",
      "\n",
      "Dequantized: tensor([[ 0.0135, -0.6463, -0.4308,  ..., -0.0187, -1.1971, -0.8978],\n",
      "        [-1.7956, -1.7956, -0.8978,  ...,  0.3425,  1.0275,  0.0107],\n",
      "        [ 0.5137, -0.6850, -0.3425,  ..., -1.5029,  0.0157, -2.0038],\n",
      "        ...,\n",
      "        [ 1.3577,  0.0141,  0.9051,  ...,  0.7068, -0.9424, -0.9424],\n",
      "        [ 0.4712, -1.8849,  0.9424,  ...,  0.3743, -0.3743,  0.7486],\n",
      "        [-0.5614, -2.2457, -0.3743,  ...,  1.5905,  0.5302,  0.7952]],\n",
      "       device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor_fp32 = torch.randn(100, 100, dtype=torch.float32).cuda()\n",
    "print(f\"Original: {tensor_fp32} \\n\")\n",
    "\n",
    "quantized_tensor = bnb.functional.quantize_4bit(tensor_fp32)\n",
    "print(f\"Quantized: {quantized_tensor[0]} \\n\")\n",
    "\n",
    "dequantized_tensor = bnb.functional.dequantize_4bit(quantized_tensor[0],quantized_tensor[1])\n",
    "print(f\"Dequantized: {dequantized_tensor} \\n\")\n",
    "\n",
    "# Move everything to CPU for easier plotting\n",
    "tensor_fp32_cpu = tensor_fp32.detach().cpu().flatten()\n",
    "quantized_tensor_cpu = quantized_tensor[0].detach().cpu().flatten()\n",
    "dequantized_tensor_cpu = dequantized_tensor.detach().cpu().flatten()\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(tensor_fp32_cpu.numpy(), bins=100, color='blue', alpha=0.7)\n",
    "plt.title('Original Tensor (FP32)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(quantized_tensor_cpu.numpy(), bins=16, color='green', alpha=0.7)\n",
    "plt.title('Quantized Tensor (4-bit)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(dequantized_tensor_cpu.numpy(), bins=100, color='red', alpha=0.7)\n",
    "plt.title('Dequantized Tensor')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA\n",
    "\n",
    "![QLORA](images/schema_QLORA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, pipeline\n",
    "import torch, gc\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 0.08 MB |||||  Reserved:  70.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 1023.27 MB |||||  Reserved:  1574.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[-5.0852e+13],\n",
       "            [ 1.2987e+22],\n",
       "            [ 8.9369e+31],\n",
       "            ...,\n",
       "            [-1.3732e-26],\n",
       "            [ 8.7257e+16],\n",
       "            [-1.5272e-34]], device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.0.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.1.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.2.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.3.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.4.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.5.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.6.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.7.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.8.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.9.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.10.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.11.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.12.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.13.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.14.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.15.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.norm.weight  dtype: torch.float16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "# Now manually move LoRA params to bf16\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "        if param.requires_grad:\n",
    "            param.grad = None  # Reset grads just in case\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.float16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}]\n",
    "\n",
    "\n",
    "tokenized_query = tokenizer.apply_chat_template(query,tokenize=True,add_generation_prompt=True)\n",
    "\n",
    "full_message = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_full_message = tokenizer.apply_chat_template(full_message,tokenize=True,add_generation_prompt=False)\n",
    "\n",
    "labels = tokenized_full_message\n",
    "labels = [-100] * len(tokenized_query) + tokenized_full_message[len(tokenized_query):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(tokenized_full_message).unsqueeze(0).to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([l for l in labels if l!=-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 1044.69 MB |||||  Reserved:  1548.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.923828125\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1618.00 MB:\n",
      "Loss: 2.021484375\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:00<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.94921875\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n",
      "Loss: 0.251953125\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.046875\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "print_gpu_utilization()\n",
    "for _ in tqdm(range(5)):\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    # flush()\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 May 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n",
    "import gc\n",
    "# flush()\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
