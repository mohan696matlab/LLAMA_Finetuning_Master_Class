{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the LLAMA 3.2 with Pure Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA is a gated model and you will need to signin in using huggingface token to download the checkpoint (Only for the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357.1290283203125\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/(1024*1024)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to ask a question to LLAMA 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Italy?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=20, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is behind the Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
       "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
       "             25,    220,   1627,   5186,    220,   2366,     20,    271, 128009,\n",
       "         128006,    882, 128007,    271,   3923,    374,    279,   6864,    315,\n",
       "          15704,     30, 128009, 128006,  78191, 128007,    271]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(tokenized_text, return_tensors=\"pt\").to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(tokenized_text, return_tensors=\"pt\",padding='max_length',max_length=300).to(device)\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 43, 128256]), torch.Size([1, 43]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,      2, 128006,    198,  78191,    567,   1303,    311,  12299,\n",
      "             25,    220,    220,     16,     15,    198,   9673,    596,     25,\n",
      "            510,   2360,   7552,    220,   2366,     18,    198,   2028, 128006,\n",
      "          78191,   3638,    271,   2028,    374,    279,   2694,    315,   1561,\n",
      "           5380,  22463, 128006,  78191, 128007,    271,    791]],\n",
      "       device='cuda:0')\n",
      "tensor(791, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "print(predicted_ids)\n",
    "\n",
    "next_word_id = predicted_ids[0, -1]\n",
    "print(next_word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next word:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The predicted next word:')\n",
    "tokenizer.decode(next_word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_ids = torch.cat([inputs.input_ids, next_word_id.view((-1,1))], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted next word:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Italy is Rome.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "That's correct. Rome has been the capital of Italy since 1871, when the Kingdom of Italy was established.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Is\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(new_input_ids)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "next_word_id = predicted_ids[0, -1]\n",
    "\n",
    "new_input_ids = torch.cat([new_input_ids, next_word_id.view((-1,1))], dim=-1)\n",
    "\n",
    "print('The predicted next word:')\n",
    "print(tokenizer.batch_decode(new_input_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens when we ask something out of it's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I can’t provide information on how BG-XAI helps in fault diagnosis. Is there anything else I\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=20, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train the model to learn this new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n",
      "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   1627,   5186,    220,   2366,     20,    271, 128009,\n",
      "         128006,    882, 128007,    271,   4438,   1587,  44111,  31650,  15836,\n",
      "           1520,    304,  14867,  23842,     30, 128009, 128006,  78191, 128007,\n",
      "            271,  38862,  31650,  15836,   5825,  41941,    369,  14867,  20492,\n",
      "           1701,    459,  18274,   9134,   6108,   1749,     11,  10695,   1304,\n",
      "          15592,   6108,  50518,    810,  49839,     13, 128009]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=False)# add_generation_prompt=False as we have already added it\n",
    "print(tokenized_text)\n",
    "\n",
    "inputs = tokenizer(tokenized_text, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift logits and labels for causal LM\n",
    "shift_logits = logits[:-1, :].contiguous()\n",
    "shift_labels = inputs[\"input_ids\"][:,1:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69, 128256]), torch.Size([1, 69]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits.shape,shift_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:05,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:01<00:02,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:01<00:01,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.400390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:01<00:01,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.357421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:02<00:01,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:02<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:02<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.33984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:02<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.333984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.train()\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = inputs[\"input_ids\"][:,1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is capital of Italy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is capital of Italy?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement-1\n",
    "### Calculate loss only on the 'RESPONSE' not on the 'QUERY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}]\n",
    "\n",
    "tokenized_query = tokenizer.apply_chat_template(query,tokenize=True,add_generation_prompt=True)\n",
    "\n",
    "full_message = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_full_message = tokenizer.apply_chat_template(full_message,tokenize=True,add_generation_prompt=False)\n",
    "\n",
    "labels = tokenized_full_message\n",
    "labels = [-100] * len(tokenized_query) + tokenized_full_message[len(tokenized_query):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(tokenized_full_message).unsqueeze(0).to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([l for l in labels if l!=-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 2480.88 MB |||||  Reserved:  7708.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:01,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00107574462890625\n",
      "GPU Memory Usage>>>> Allocated: 7197.60 MB |||||  Reserved:  11992.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:01,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.671875\n",
      "GPU Memory Usage>>>> Allocated: 7197.60 MB |||||  Reserved:  12494.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.296875\n",
      "GPU Memory Usage>>>> Allocated: 7197.60 MB |||||  Reserved:  12494.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:01<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.060791015625\n",
      "GPU Memory Usage>>>> Allocated: 7197.60 MB |||||  Reserved:  12494.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.027099609375\n",
      "GPU Memory Usage>>>> Allocated: 7197.60 MB |||||  Reserved:  12494.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.train()\n",
    "print_gpu_utilization()\n",
    "for _ in tqdm(range(5)):\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement-2\n",
    "### Using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch, gc\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 0.00 MB |||||  Reserved:  0.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 2357.13 MB |||||  Reserved:  2862.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "model.norm.weight  dtype: torch.bfloat16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "# Now manually move LoRA params to bf16\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "        if param.requires_grad:\n",
    "            param.grad = None  # Reset grads just in case\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.bfloat16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}]\n",
    "\n",
    "tokenized_query = tokenizer.apply_chat_template(query,tokenize=True,add_generation_prompt=True)\n",
    "\n",
    "full_message = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_full_message = tokenizer.apply_chat_template(full_message,tokenize=True,add_generation_prompt=False)\n",
    "\n",
    "labels = tokenized_full_message\n",
    "labels = [-100] * len(tokenized_query) + tokenized_full_message[len(tokenized_query):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(tokenized_full_message).unsqueeze(0).to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([l for l in labels if l!=-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 2411.76 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00634765625\n",
      "GPU Memory Usage>>>> Allocated: 2454.76 MB |||||  Reserved:  3022.00 MB:\n",
      "Loss: 0.0201416015625\n",
      "GPU Memory Usage>>>> Allocated: 2454.76 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0026092529296875\n",
      "GPU Memory Usage>>>> Allocated: 2454.76 MB |||||  Reserved:  3022.00 MB:\n",
      "Loss: 0.00093841552734375\n",
      "GPU Memory Usage>>>> Allocated: 2454.76 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0072021484375\n",
      "GPU Memory Usage>>>> Allocated: 2454.76 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "print_gpu_utilization()\n",
    "for _ in tqdm(range(5)):\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    # flush()\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
      "GPU Memory Usage>>>> Allocated: 2454.76 MB |||||  Reserved:  3022.00 MB:\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n",
    "import gc\n",
    "# flush()\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement 3\n",
    "\n",
    "### Using Q-LoRA (Quantized in 4 bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, pipeline\n",
    "import torch, gc\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)\n",
    "    print(f\"GPU Memory Usage>>>> Allocated: {allocated:.2f} MB |||||  Reserved:  {reserved:.2f} MB:\")\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 0.00 MB |||||  Reserved:  0.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 1023.18 MB |||||  Reserved:  1506.00 MB:\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.0.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.0.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.0.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.1.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.1.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.1.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.2.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.2.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.2.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.3.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.3.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.3.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.4.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.4.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.4.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.5.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.5.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.5.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.6.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.6.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.6.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.7.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.7.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.7.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.8.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.8.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.8.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.9.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.9.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.9.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.10.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.10.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.10.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.11.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.11.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.11.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.12.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.12.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.12.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.13.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.13.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.13.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.14.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.14.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.14.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.15.self_attn.q_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.self_attn.k_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.self_attn.v_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.self_attn.o_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.mlp.gate_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.mlp.up_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.mlp.down_proj.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "model.layers.15.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.15.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.norm.weight  dtype: torch.float16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "# Now manually move LoRA params to bf16\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.data = param.data.to(torch.bfloat16)\n",
    "        if param.requires_grad:\n",
    "            param.grad = None  # Reset grads just in case\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.bfloat16  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.bfloat16  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.float16  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "query = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}]\n",
    "\n",
    "\n",
    "tokenized_query = tokenizer.apply_chat_template(query,tokenize=True,add_generation_prompt=True)\n",
    "\n",
    "full_message = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\"}\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_full_message = tokenizer.apply_chat_template(full_message,tokenize=True,add_generation_prompt=False)\n",
    "\n",
    "labels = tokenized_full_message\n",
    "labels = [-100] * len(tokenized_query) + tokenized_full_message[len(tokenized_query):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(tokenized_full_message).unsqueeze(0).to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([l for l in labels if l!=-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory Usage>>>> Allocated: 1044.69 MB |||||  Reserved:  1548.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:01,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.943359375\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1618.00 MB:\n",
      "Loss: 2.072265625\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:01<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.78369140625\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n",
      "Loss: 0.158447265625\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0286407470703125\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "\n",
    "print_gpu_utilization()\n",
    "for _ in tqdm(range(5)):\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits[0]\n",
    "    # Shift logits and labels for causal LM\n",
    "    shift_logits = logits[:-1, :].contiguous()\n",
    "    shift_labels = labels[1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "    shift_logits,\n",
    "    shift_labels.view(-1),\n",
    ")\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    # flush()\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "c:\\Users\\User\\miniconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How does BG-XAI help in fault diagnosis?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
      "GPU Memory Usage>>>> Allocated: 1120.82 MB |||||  Reserved:  1666.00 MB:\n"
     ]
    }
   ],
   "source": [
    "# Q: What is BG-CNN and what does it do?\n",
    "# A: BG-CNN is a hybrid FDI method that combines Bond Graph residual generation with CNN-based fault classification, designed to work well even with limited labeled data.\n",
    "\n",
    "# Q: How does BG-XAI help in fault diagnosis?\n",
    "# A: BG-XAI provides explanations for fault predictions using an occlusion-based method, helping make AI-based diagnostics more understandable.\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How does BG-XAI help in fault diagnosis?\"}\n",
    "]\n",
    "\n",
    "tokenized_text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generated_text = llama_pipeline(tokenized_text, max_new_tokens=60, early_stopping=True)\n",
    "\n",
    "print(generated_text[0]['generated_text'])\n",
    "\n",
    "import gc\n",
    "# flush()\n",
    "print_gpu_utilization()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
